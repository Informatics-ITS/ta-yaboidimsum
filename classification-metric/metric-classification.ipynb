{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-06-07T08:05:58.2324Z",
          "iopub.status.busy": "2025-06-07T08:05:58.23192Z",
          "iopub.status.idle": "2025-06-07T08:07:32.468652Z",
          "shell.execute_reply": "2025-06-07T08:07:32.468019Z",
          "shell.execute_reply.started": "2025-06-07T08:05:58.232375Z"
        },
        "id": "vpLZrPRIHGrE",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opencv-python) (1.25.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
            "[notice] To update, run: C:\\Users\\dprih\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
            "[notice] To update, run: C:\\Users\\dprih\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
            "[notice] To update, run: C:\\Users\\dprih\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gdown\n",
            "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gdown) (4.12.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
            "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\dprih\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->gdown) (0.4.6)\n",
            "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
            "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: PySocks, gdown\n",
            "Successfully installed PySocks-1.7.1 gdown-5.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'timm'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms, models\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m  \u001b[38;5;66;03m# For EfficientNetV2 models\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Image processing\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'timm'"
          ]
        }
      ],
      "source": [
        "%pip install opencv-python\n",
        "%pip install timm  # For EfficientNetV2 models\n",
        "%pip install gdown\n",
        "\n",
        "# Basic imports\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import timm  # For EfficientNetV2 models\n",
        "\n",
        "# Image processing\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrzPEJWAHGrI"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgo2NW1gHGrK"
      },
      "source": [
        "Clone the dataset from the repository using Git."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:07:32.470212Z",
          "iopub.status.busy": "2025-06-07T08:07:32.469793Z",
          "iopub.status.idle": "2025-06-07T08:07:51.589891Z",
          "shell.execute_reply": "2025-06-07T08:07:51.589189Z",
          "shell.execute_reply.started": "2025-06-07T08:07:32.47019Z"
        },
        "id": "wTgX3SVgHGrK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # Normal tidak di white balance\n",
        "# !gdown https://drive.google.com/uc?id=1JUbef73bC02xEeGZveCBQ0HOSM-Q9zGU\n",
        "# !gdown https://drive.google.com/uc?id=1P_ry-xorev301eMAr10a_Cht9Si7BDC9\n",
        "# !gdown https://drive.google.com/uc?id=1qmf7YizdXe2XhPIsA43onUVn-MYYYDVw\n",
        "# !unzip \"Data_Asli.zip\" -d dataset_asli\n",
        "\n",
        "# # Dilakukan old contrast enhancement\n",
        "# !gdown https://drive.google.com/uc?id=1_qa3nGhPM7_LnzBRFm1XVZbg-ypYS8M6\n",
        "# !gdown https://drive.google.com/uc?id=1XTFp5v0UJe6wy3II4hwqWvV8exkIJSpp\n",
        "# !gdown https://drive.google.com/uc?id=1cMWFtBrePO08hjHOdYZE7PRZV-zZfK52\n",
        "# 10.000 data ASLI GAN RATIO\n",
        "# !gdown https://drive.google.com/uc?id=1s7Cu9chsRrI30Qkl5u13VuqPwWysfNsr\n",
        "\n",
        "# Dilakukan NEW contrast enhancement FILTER\n",
        "!gdown https://drive.google.com/uc?id=1BHrO8ZM08NqGFBnZjHDlmSYFYg7f8Z1U\n",
        "!gdown https://drive.google.com/uc?id=1x6JVhPgO2Fq9yNGK57iD4RPflKdpM3vI\n",
        "!gdown https://drive.google.com/uc?id=1MfKxrt1YzIRVdlUxW881hrEBIx335gUE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:07:51.591224Z",
          "iopub.status.busy": "2025-06-07T08:07:51.590965Z",
          "iopub.status.idle": "2025-06-07T08:07:54.994956Z",
          "shell.execute_reply": "2025-06-07T08:07:54.994271Z",
          "shell.execute_reply.started": "2025-06-07T08:07:51.591186Z"
        },
        "id": "NH4AF2CDHGrL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !unzip \"Dataset_Asli.zip\" -d dataset_asli\n",
        "# !unzip \"Dataset_Asli_GAN.zip\" -d dataset_asli_gan\n",
        "# !unzip \"Dataset_Asli_GAN_Ratio.zip\" -d dataset_asli_gan_ratio\n",
        "\n",
        "!unzip \"Dataset_Asli_Enhanced.zip\" -d dataset_asli_enhanced\n",
        "!unzip \"Dataset_Asli_GAN_Enhanced.zip\" -d dataset_asli_gan_enhanced\n",
        "!unzip \"Dataset_Asli_GAN_Ratio_Enhanced.zip\" -d dataset_asli_gan_ratio_enhanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:07:54.997416Z",
          "iopub.status.busy": "2025-06-07T08:07:54.997089Z",
          "iopub.status.idle": "2025-06-07T08:07:55.08774Z",
          "shell.execute_reply": "2025-06-07T08:07:55.08689Z",
          "shell.execute_reply.started": "2025-06-07T08:07:54.997393Z"
        },
        "id": "HGIwCCg0HGrM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "BATCH_SIZE = 32  # Reduced for better stability\n",
        "IMAGE_SIZE = (224, 224)  # EfficientNetV2B0 default input size\n",
        "NUM_CLASSES = 3  # L1, L2, L3\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Dataset paths\n",
        "# Normal tidak di white balance\n",
        "# BASE_DIR = './dataset_asli/Dataset_Asli'  # Current directory where L1, L2, L3 folders are located\n",
        "# BASE_DIR_GAN = './dataset_asli_gan/Dataset_Asli_GAN'  # Current directory where L1, L2, L3 folders are located\n",
        "# BASE_DIR_GAN_RATIO = './dataset_asli_gan_ratio/Dataset_Asli_GAN_Ratio'  # Current directory where L1, L2, L3 folders are located\n",
        "\n",
        "# Contrast Enhancement\n",
        "BASE_DIR = './dataset_asli_enhanced/Dataset_Asli_Enhanced'  # Current directory where L1, L2, L3 folders are located\n",
        "BASE_DIR_GAN = './dataset_asli_gan_enhanced/Dataset_Asli_GAN_Enhanced'  # Current directory where L1, L2, L3 folders are located\n",
        "BASE_DIR_GAN_RATIO = './dataset_asli_gan_ratio_enhanced/Dataset_Asli_GAN_Ratio_Enhanced'  # Current directory where L1, L2, L3 folders are located\n",
        "\n",
        "CLASSES = ['L1', 'L2', 'L3']\n",
        "\n",
        "# Training parameters\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "NUM_EPOCHS = 100\n",
        "EARLY_STOPPING_PATIENCE_5 = 5\n",
        "EARLY_STOPPING_PATIENCE_10 = 10\n",
        "EARLY_STOPPING_PATIENCE_15 = 15\n",
        "EARLY_STOPPING_PATIENCE_20 = 20\n",
        "EARLY_STOPPING_PATIENCE_NONE = None\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:07:55.088955Z",
          "iopub.status.busy": "2025-06-07T08:07:55.088684Z",
          "iopub.status.idle": "2025-06-07T08:07:55.104145Z",
          "shell.execute_reply": "2025-06-07T08:07:55.103502Z",
          "shell.execute_reply.started": "2025-06-07T08:07:55.08893Z"
        },
        "id": "DDP-TvNWHGrN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import WeightedRandomSampler\n",
        "import numpy as np\n",
        "import os # Pastikan os diimpor\n",
        "\n",
        "def create_weighted_dataloader(dataset, batch_size, num_classes, num_workers=0, original_data_weight_multiplier=2.0):\n",
        "    \"\"\"\n",
        "    Membuat DataLoader dengan WeightedRandomSampler untuk menangani ketidakseimbangan kelas\n",
        "    dan memberikan bobot berbeda pada data asli vs. data GAN, berdasarkan awalan nama file \"seed\".\n",
        "\n",
        "    Args:\n",
        "        dataset (torch.utils.data.Dataset): Dataset untuk dibuat DataLoader-nya.\n",
        "        batch_size (int): Ukuran batch.\n",
        "        num_classes (int): Jumlah total kelas.\n",
        "        num_workers (int): Jumlah worker untuk loading data.\n",
        "        original_data_weight_multiplier (float): Multiplier bobot untuk data asli. Default 2.0.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.data.DataLoader: DataLoader dengan WeightedRandomSampler.\n",
        "    \"\"\"\n",
        "    if not hasattr(dataset, 'labels') or not dataset.labels:\n",
        "        print(f\"Warning: Dataset {dataset.split} has no 'labels' attribute or it's empty. Cannot create weighted sampler. Using shuffle=True.\")\n",
        "        return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    labels = dataset.labels\n",
        "    image_paths = dataset.data # Asumsi dataset.data adalah list of image paths\n",
        "\n",
        "    # Hitung jumlah setiap kelas\n",
        "    class_counts = np.bincount(labels, minlength=num_classes)\n",
        "    class_counts = np.maximum(class_counts, 1)\n",
        "    class_weights_base = 1.0 / class_counts\n",
        "\n",
        "    sample_weights = []\n",
        "    # Definisi awalan nama file untuk gambar GAN\n",
        "    gan_file_prefix = \"seed\"\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        weight = class_weights_base[label]\n",
        "\n",
        "        img_filename = os.path.basename(image_paths[i]) # Ambil nama file dari jalur lengkap\n",
        "\n",
        "        # Deteksi apakah ini gambar GAN berdasarkan awalan nama file\n",
        "        is_gan_image = img_filename.startswith(gan_file_prefix)\n",
        "\n",
        "        if not is_gan_image: # Jika ini adalah gambar ASLI (tidak diawali \"seed\")\n",
        "            weight *= original_data_weight_multiplier # Beri bobot ekstra pada data asli\n",
        "\n",
        "        sample_weights.append(weight)\n",
        "\n",
        "    sample_weights = torch.DoubleTensor(sample_weights)\n",
        "\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=sample_weights,\n",
        "        num_samples=len(sample_weights),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=sampler,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:08:14.346701Z",
          "iopub.status.busy": "2025-06-07T08:08:14.346252Z",
          "iopub.status.idle": "2025-06-07T08:08:32.498129Z",
          "shell.execute_reply": "2025-06-07T08:08:32.497524Z",
          "shell.execute_reply.started": "2025-06-07T08:08:14.346681Z"
        },
        "id": "cAekscuLHGrO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(RANDOM_SEED)\n",
        "\n",
        "# Definisikan model dummy untuk EfficientNetV2-B0\n",
        "dummy_model_effnetv2 = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=True)\n",
        "# Definisikan model dummy untuk ResNet50\n",
        "dummy_model_resnet50 = timm.create_model('resnet50', pretrained=True)\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    CLASS_NAMES = ['L1', 'L2', 'L3']\n",
        "    CLASS_TO_LABEL = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
        "\n",
        "    def __init__(self, root_dir=None, split='train', print_info=True, model_name_for_transforms=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.image_sizes = {}\n",
        "\n",
        "        # Ini yang baru: mendapatkan transform sesuai nama model\n",
        "        if model_name_for_transforms == 'efficientnetv2-training':\n",
        "            data_config = timm.data.resolve_model_data_config(dummy_model_effnetv2)\n",
        "            self.transform = timm.data.create_transform(**data_config, is_training=True)\n",
        "        elif model_name_for_transforms == 'efficientnetv2':\n",
        "            data_config = timm.data.resolve_model_data_config(dummy_model_effnetv2)\n",
        "            self.transform = timm.data.create_transform(**data_config, is_training=False)\n",
        "        elif model_name_for_transforms == 'resnet50-training':\n",
        "            data_config = timm.data.resolve_model_data_config(dummy_model_resnet50)\n",
        "            self.transform = timm.data.create_transform(**data_config, is_training=True)\n",
        "        elif model_name_for_transforms == 'resnet50':\n",
        "            data_config = timm.data.resolve_model_data_config(dummy_model_resnet50)\n",
        "            self.transform = timm.data.create_transform(**data_config, is_training=False)\n",
        "        elif model_name_for_transforms == 'None':\n",
        "            print(\"Pretrained False\")\n",
        "            # self.transform = transforms.Compose([\n",
        "            #     transforms.Resize((256, 256)),\n",
        "            #     transforms.ToTensor(),\n",
        "            #     # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            # ])\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((256, 256)), # Ukuran yang konsisten adalah penting\n",
        "                # transforms.RandomHorizontalFlip(),  # Flip horizontal\n",
        "                # transforms.RandomRotation(degrees=15), # Rotasi acak hingga 15 derajat\n",
        "                # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Perubahan warna\n",
        "                # transforms.RandomRotation(degrees=(-45, 45)),\n",
        "                # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "                # transforms.RandomAffine(degrees=(-10, 10), translate=(0.1, 0.1), scale=(0.9, 1.1), shear=(-5, 5)),\n",
        "                # transforms.GaussianBlur(kernel_size=(3, 3)), # Pertimbangkan jika dataset Anda cenderung buram\n",
        "                # transforms.RandomPerspective(distortion_scale=0.2, p=0.2), # Gunakan dengan hati-hati\n",
        "                transforms.ToTensor(), # Ubah ke tensor\n",
        "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalisasi setelah ToTensor\n",
        "            ])\n",
        "        else:\n",
        "            # Fallback transform jika model_name_for_transforms tidak dikenali\n",
        "            print(\"Warning: Using default transform. Specify 'efficientnetv2_b0' or 'resnet50' for model_name_for_transforms.\")\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalisasi setelah ToTensor\n",
        "            ])\n",
        "\n",
        "        if root_dir:\n",
        "            start = time.time()\n",
        "            self._load_images()\n",
        "            if print_info:\n",
        "                self._print_info(round(time.time() - start, 2))\n",
        "\n",
        "    def _load_images(self):\n",
        "        for class_name, label in self.CLASS_TO_LABEL.items():\n",
        "            # Construct path to the specific split folder (train/val/test) for each class\n",
        "            class_dir = os.path.join(self.root_dir, class_name, self.split)\n",
        "            if not os.path.exists(class_dir):\n",
        "                continue\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                img_path = os.path.join(class_dir, img_name)\n",
        "                try:\n",
        "                    with Image.open(img_path) as img:\n",
        "                        img = img.convert(\"RGB\")\n",
        "                        self.image_sizes[img.size] = self.image_sizes.get(img.size, 0) + 1\n",
        "                    self.data.append(img_path)\n",
        "                    self.labels.append(label)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to load image {img_path}: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "# Load and transform image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "\n",
        "        # image = Image.open(img_path).convert(\"RGB\")\n",
        "        return image, label\n",
        "\n",
        "    def to_dataframe(self):\n",
        "        return pd.DataFrame({\n",
        "            'image_path': self.data,\n",
        "            'label': self.labels\n",
        "        })\n",
        "\n",
        "    def _print_info(self, elapsed):\n",
        "        print(\"========== Dataset Summary ==========\")\n",
        "        print(f\"Split: {self.split}\")\n",
        "        print(f\"Total images       : {len(self.data)}\")\n",
        "        print(f\"Number of classes  : {len(self.CLASS_NAMES)}\")\n",
        "\n",
        "        # Images per class table\n",
        "        print(\"\\n--- Images per Class ---\")\n",
        "        print(f\"{'Label':<7} | {'Class Name':<25} | {'Image Count':>12}\")\n",
        "        print(\"-\" * 50)\n",
        "        class_counts = {name: 0 for name in self.CLASS_NAMES}\n",
        "        for label in self.labels:\n",
        "            class_name = self.CLASS_NAMES[label]\n",
        "            class_counts[class_name] += 1\n",
        "        for label, class_name in enumerate(self.CLASS_NAMES):\n",
        "            count = class_counts[class_name]\n",
        "            print(f\"{label:<7} | {class_name:<25} | {count:>12}\")\n",
        "\n",
        "        # Unique image sizes\n",
        "        print(\"\\n--- Unique Image Sizes ---\")\n",
        "        print(f\"{'Image Size (WxH)':<25} | {'Count':>12}\")\n",
        "        print(\"-\" * 40)\n",
        "        for size, count in sorted(self.image_sizes.items(), key=lambda x: -x[1]):\n",
        "            size_str = f\"{size[0]}x{size[1]}\"\n",
        "            print(f\"{size_str:<25} | {count:>12}\")\n",
        "\n",
        "        print(f\"\\nLoaded in {elapsed:.2f} seconds ✅\")\n",
        "\n",
        "# Define image transformations\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),  # EfficientNetV2B0 default input size\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
        "# ])\n",
        "\n",
        "# Initialize datasets for each split\n",
        "print('--------------------------------------------------------------------')\n",
        "print('--------------------------- Pure Dataset ---------------------------')\n",
        "print('--------------------------------------------------------------------')\n",
        "print('---------------------------Data Asli---------------------------')\n",
        "train_dataset_asli = Dataset(BASE_DIR, split='train',  model_name_for_transforms='None')\n",
        "val_dataset_asli = Dataset(BASE_DIR, split='val',   model_name_for_transforms=None)\n",
        "test_dataset_asli = Dataset(BASE_DIR, split='test',   model_name_for_transforms=None)\n",
        "print('---------------------------Data Asli + GAN---------------------------')\n",
        "train_dataset_asli_gan = Dataset(BASE_DIR_GAN, split='train',  model_name_for_transforms='None')\n",
        "val_dataset_asli_gan = Dataset(BASE_DIR_GAN, split='val',   model_name_for_transforms=None)\n",
        "test_dataset_asli_gan = Dataset(BASE_DIR_GAN, split='test',   model_name_for_transforms=None)\n",
        "print('---------------------------Data Asli + GAN + Ratio---------------------------')\n",
        "train_dataset_asli_gan_ratio = Dataset(BASE_DIR_GAN_RATIO, split='train',  model_name_for_transforms='None')\n",
        "val_dataset_asli_gan_ratio = Dataset(BASE_DIR_GAN_RATIO, split='val',   model_name_for_transforms=None)\n",
        "test_dataset_asli_gan_ratio = Dataset(BASE_DIR_GAN_RATIO, split='test',   model_name_for_transforms=None)\n",
        "\n",
        "print('-----------------------------------------------------------------------------------')\n",
        "print('--------------------------- (Resnet) Pretrained Dataset ---------------------------')\n",
        "print('-----------------------------------------------------------------------------------')\n",
        "print('---------------------------Data Asli---------------------------')\n",
        "train_dataset_asli_pretrained_resnet = Dataset(BASE_DIR, split='train',  model_name_for_transforms='resnet50-training')\n",
        "val_dataset_asli_pretrained_resnet= Dataset(BASE_DIR, split='val',   model_name_for_transforms='resnet50')\n",
        "test_dataset_asli_pretrained_resnet = Dataset(BASE_DIR, split='test',   model_name_for_transforms='resnet50')\n",
        "print('---------------------------Data Asli + GAN---------------------------')\n",
        "train_dataset_asli_gan_pretrained_resnet = Dataset(BASE_DIR_GAN, split='train',  model_name_for_transforms='resnet50-training')\n",
        "val_dataset_asli_gan_pretrained_resnet = Dataset(BASE_DIR_GAN, split='val',   model_name_for_transforms='resnet50')\n",
        "test_dataset_asli_gan_pretrained_resnet = Dataset(BASE_DIR_GAN, split='test',   model_name_for_transforms='resnet50')\n",
        "print('---------------------------Data Asli + GAN + Ratio---------------------------')\n",
        "train_dataset_asli_gan_ratio_pretrained_resnet = Dataset(BASE_DIR_GAN_RATIO, split='train',  model_name_for_transforms='resnet50-training')\n",
        "val_dataset_asli_gan_ratio_pretrained_resnet = Dataset(BASE_DIR_GAN_RATIO, split='val',   model_name_for_transforms='resnet50')\n",
        "test_dataset_asli_gan_ratio_pretrained_resnet = Dataset(BASE_DIR_GAN_RATIO, split='test',   model_name_for_transforms='resnet50')\n",
        "\n",
        "print('-----------------------------------------------------------------------------------')\n",
        "print('--------------------------- (EfficientNetV2) Pretrained Dataset ---------------------------')\n",
        "print('-----------------------------------------------------------------------------------')\n",
        "print('---------------------------Data Asli---------------------------')\n",
        "train_dataset_asli_pretrained_efficientnet = Dataset(BASE_DIR, split='train',  model_name_for_transforms='efficientnetv2-training')\n",
        "val_dataset_asli_pretrained_efficientnet= Dataset(BASE_DIR, split='val',   model_name_for_transforms='efficientnetv2')\n",
        "test_dataset_asli_pretrained_efficientnet = Dataset(BASE_DIR, split='test',   model_name_for_transforms='efficientnetv2')\n",
        "print('---------------------------Data Asli + GAN---------------------------')\n",
        "train_dataset_asli_gan_pretrained_efficientnet = Dataset(BASE_DIR_GAN, split='train',  model_name_for_transforms='efficientnetv2-training')\n",
        "val_dataset_asli_gan_pretrained_efficientnet = Dataset(BASE_DIR_GAN, split='val',   model_name_for_transforms='efficientnetv2')\n",
        "test_dataset_asli_gan_pretrained_efficientnet = Dataset(BASE_DIR_GAN, split='test',   model_name_for_transforms='efficientnetv2')\n",
        "print('---------------------------Data Asli + GAN + Ratio---------------------------')\n",
        "train_dataset_asli_gan_ratio_pretrained_efficientnet = Dataset(BASE_DIR_GAN_RATIO, split='train',  model_name_for_transforms='efficientnetv2-training')\n",
        "val_dataset_asli_gan_ratio_pretrained_efficientnet = Dataset(BASE_DIR_GAN_RATIO, split='val',   model_name_for_transforms='efficientnetv2')\n",
        "test_dataset_asli_gan_ratio_pretrained_efficientnet = Dataset(BASE_DIR_GAN_RATIO, split='test',   model_name_for_transforms='efficientnetv2')\n",
        "\n",
        "# Create data loaders\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "print('---------------------------------------------------------------------------')\n",
        "print('--------------------------- Pure Dataset Loader ---------------------------')\n",
        "print('---------------------------------------------------------------------------')\n",
        "# print('---------------------------Data Asli---------------------------')\n",
        "# train_loader_asli = create_weighted_dataloader(train_dataset_asli, BATCH_SIZE, NUM_CLASSES, num_workers=0)\n",
        "train_loader_asli = DataLoader(train_dataset_asli, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader_asli = DataLoader(val_dataset_asli, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader_asli = DataLoader(test_dataset_asli, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "# print('---------------------------Data Asli + GAN---------------------------')\n",
        "# train_loader_asli_gan = create_weighted_dataloader(train_dataset_asli_gan, BATCH_SIZE, NUM_CLASSES, num_workers=0)\n",
        "train_loader_asli_gan = DataLoader(train_dataset_asli_gan, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader_asli_gan = DataLoader(val_dataset_asli_gan, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader_asli_gan = DataLoader(test_dataset_asli_gan, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "# print('---------------------------Data Asli + GAN + Ratio---------------------------')\n",
        "# train_loader_asli_gan_ratio = create_weighted_dataloader(train_dataset_asli_gan_ratio, BATCH_SIZE, NUM_CLASSES, num_workers=0)\n",
        "train_loader_asli_gan_ratio = DataLoader(train_dataset_asli_gan_ratio, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader_asli_gan_ratio = DataLoader(val_dataset_asli_gan_ratio, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader_asli_gan_ratio = DataLoader(test_dataset_asli_gan_ratio, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print('-----------------------------------------------------------------------------------')\n",
        "print('--------------------------- (Resnet) Pretrained Dataset ---------------------------')\n",
        "print('-----------------------------------------------------------------------------------')\n",
        "\n",
        "# print('---------------------------Data Asli---------------------------')\n",
        "# train_loader_asli_pretrained_resnet = create_weighted_dataloader(train_dataset_asli_pretrained_resnet, BATCH_SIZE, NUM_CLASSES, num_workers=0)\n",
        "train_loader_asli_pretrained_resnet = DataLoader(train_dataset_asli_pretrained_resnet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader_asli_pretrained_resnet = DataLoader(val_dataset_asli_pretrained_resnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader_asli_pretrained_resnet = DataLoader(test_dataset_asli_pretrained_resnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "# print('---------------------------Data Asli + GAN---------------------------')\n",
        "# train_loader_asli_gan_pretrained_resnet = create_weighted_dataloader(train_dataset_asli_gan_pretrained_resnet, BATCH_SIZE, NUM_CLASSES, num_workers=0)\n",
        "train_loader_asli_gan_pretrained_resnet = DataLoader(train_dataset_asli_gan_pretrained_resnet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader_asli_gan_pretrained_resnet = DataLoader(val_dataset_asli_gan_pretrained_resnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader_asli_gan_pretrained_resnet = DataLoader(test_dataset_asli_gan_pretrained_resnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "# print('---------------------------Data Asli + GAN + Ratio---------------------------')\n",
        "# train_loader_asli_gan_ratio_pretrained_resnet = create_weighted_dataloader(train_dataset_asli_gan_ratio_pretrained_resnet, BATCH_SIZE, NUM_CLASSES, num_workers=0)\n",
        "train_loader_asli_gan_ratio_pretrained_resnet = DataLoader(train_dataset_asli_gan_ratio_pretrained_resnet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader_asli_gan_ratio_pretrained_resnet = DataLoader(val_dataset_asli_gan_ratio_pretrained_resnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader_asli_gan_ratio_pretrained_resnet = DataLoader(test_dataset_asli_gan_ratio_pretrained_resnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print('----------------------------------------------------------------------------------------------')\n",
        "print('--------------------------- (EfficientNetV2_B0) Pretrained Dataset ---------------------------')\n",
        "print('----------------------------------------------------------------------------------------------')\n",
        "\n",
        "# print('---------------------------Data Asli---------------------------')\n",
        "# train_loader_asli_pretrained_efficientnet = create_weighted_dataloader(train_dataset_asli_pretrained_efficientnet, BATCH_SIZE, NUM_CLASSES, num_workers=0)\n",
        "train_loader_asli_pretrained_efficientnet = DataLoader(train_dataset_asli_pretrained_efficientnet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader_asli_pretrained_efficientnet = DataLoader(val_dataset_asli_pretrained_efficientnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader_asli_pretrained_efficientnet = DataLoader(test_dataset_asli_pretrained_efficientnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "# print('---------------------------Data Asli + GAN---------------------------')\n",
        "# train_loader_asli_gan_pretrained_efficientnet = create_weighted_dataloader(train_dataset_asli_gan_pretrained_efficientnet, BATCH_SIZE, NUM_CLASSES, num_workers=0)\n",
        "train_loader_asli_gan_pretrained_efficientnet = DataLoader(train_dataset_asli_gan_pretrained_efficientnet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader_asli_gan_pretrained_efficientnet = DataLoader(val_dataset_asli_gan_pretrained_efficientnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader_asli_gan_pretrained_efficientnet = DataLoader(test_dataset_asli_gan_pretrained_efficientnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "# print('---------------------------Data Asli + GAN + Ratio---------------------------')\n",
        "# train_loader_asli_gan_ratio_pretrained_efficientnet = create_weighted_dataloader(train_dataset_asli_gan_ratio_pretrained_efficientnet, BATCH_SIZE, NUM_CLASSES, num_workers=0)\n",
        "train_loader_asli_gan_ratio_pretrained_efficientnet = DataLoader(train_dataset_asli_gan_ratio_pretrained_efficientnet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader_asli_gan_ratio_pretrained_efficientnet = DataLoader(val_dataset_asli_gan_ratio_pretrained_efficientnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader_asli_gan_ratio_pretrained_efficientnet = DataLoader(test_dataset_asli_gan_ratio_pretrained_efficientnet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os1F6fC1HGrQ"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:08:32.499295Z",
          "iopub.status.busy": "2025-06-07T08:08:32.498957Z",
          "iopub.status.idle": "2025-06-07T08:08:35.921246Z",
          "shell.execute_reply": "2025-06-07T08:08:35.920611Z",
          "shell.execute_reply.started": "2025-06-07T08:08:32.499275Z"
        },
        "id": "lHZEBronHGrR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_class_distribution(dataset, title=\"Image Distribution per Class\", sort_by_count=True, show_percentage=True):\n",
        "    \"\"\"\n",
        "    Generates a clean and informative bar chart of class distribution.\n",
        "\n",
        "    Args:\n",
        "        dataset (WasteDataset): The dataset to plot distribution for\n",
        "        title (str): Chart title.\n",
        "        sort_by_count (bool): If True, sort bars by image count descending.\n",
        "        show_percentage (bool): If True, show % along with count.\n",
        "    \"\"\"\n",
        "    # Get counts for each class\n",
        "    label_counts = Counter(dataset.labels)\n",
        "    total = sum(label_counts.values())\n",
        "\n",
        "    # Convert to list of tuples (label, count)\n",
        "    label_data = list(label_counts.items())\n",
        "\n",
        "    # Sort by count if enabled\n",
        "    if sort_by_count:\n",
        "        label_data.sort(key=lambda x: x[1], reverse=True)\n",
        "    else:\n",
        "        label_data.sort(key=lambda x: x[0])  # label order\n",
        "\n",
        "    class_names = [dataset.CLASS_NAMES[lbl] for lbl, _ in label_data]\n",
        "    counts = [cnt for _, cnt in label_data]\n",
        "    percentages = [cnt / total * 100 for cnt in counts]\n",
        "\n",
        "    # Set seaborn style\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    palette = sns.color_palette(\"pastel\")\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(class_names, counts, color=palette)\n",
        "\n",
        "    # Add count & percentage labels\n",
        "    for i, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        pct_text = f\"{height:,}\" + (f\" ({percentages[i]:.1f}%)\" if show_percentage else \"\")\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, height + 5, pct_text,\n",
        "                 ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    # Labels and title\n",
        "    plt.title(f\"{title} - {dataset.split.capitalize()} Split\", fontsize=16, weight='bold')\n",
        "    plt.xlabel(\"Class\", fontsize=12)\n",
        "    plt.ylabel(\"Number of Images\", fontsize=12)\n",
        "    plt.xticks(rotation=0, fontsize=11)\n",
        "    plt.yticks(fontsize=11)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot distribution for each split\n",
        "print('---------------------------Data Asli---------------------------')\n",
        "print(\"Training Set Distribution:\")\n",
        "plot_class_distribution(\n",
        "    train_dataset_asli,\n",
        "    title=\"Image Distribution per Class\",\n",
        "    sort_by_count=False,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nValidation Set Distribution:\")\n",
        "plot_class_distribution(\n",
        "    val_dataset_asli,\n",
        "    title=\"Image Distribution per Class\",\n",
        "    sort_by_count=False,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nTest Set Distribution:\")\n",
        "plot_class_distribution(\n",
        "    test_dataset_asli,\n",
        "    title=\"Image Distribution per Class\",\n",
        "    sort_by_count=False,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "# Plot sorted distributions\n",
        "print(\"\\nTraining Set Distribution (Sorted):\")\n",
        "plot_class_distribution(\n",
        "    train_dataset_asli,\n",
        "    title=\"Image Distribution per Class\",\n",
        "    sort_by_count=True,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nValidation Set Distribution (Sorted):\")\n",
        "plot_class_distribution(\n",
        "    val_dataset_asli,\n",
        "    title=\"Image Distribution per Class\",\n",
        "    sort_by_count=True,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nTest Set Distribution (Sorted):\")\n",
        "plot_class_distribution(\n",
        "    test_dataset_asli,\n",
        "    title=\"Image Distribution per Class\",\n",
        "    sort_by_count=True,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print('---------------------------Data Asli + GAN---------------------------')\n",
        "print(\"Training Set Distribution:\")\n",
        "plot_class_distribution(\n",
        "    train_dataset_asli_gan,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN)\",\n",
        "    sort_by_count=False,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nValidation Set Distribution:\")\n",
        "plot_class_distribution(\n",
        "    val_dataset_asli_gan,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN)\",\n",
        "    sort_by_count=False,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nTest Set Distribution:\")\n",
        "plot_class_distribution(\n",
        "    test_dataset_asli_gan,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN)\",\n",
        "    sort_by_count=False,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "# Plot sorted distributions\n",
        "print(\"\\nTraining Set Distribution (Sorted):\")\n",
        "plot_class_distribution(\n",
        "    train_dataset_asli_gan,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN)\",\n",
        "    sort_by_count=True,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nValidation Set Distribution (Sorted):\")\n",
        "plot_class_distribution(\n",
        "    val_dataset_asli_gan,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN)\",\n",
        "    sort_by_count=True,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nTest Set Distribution (Sorted):\")\n",
        "plot_class_distribution(\n",
        "    test_dataset_asli_gan,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN)\",\n",
        "    sort_by_count=True,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print('---------------------------Data Asli + GAN + Ratio---------------------------')\n",
        "print(\"Training Set Distribution:\")\n",
        "plot_class_distribution(\n",
        "    train_dataset_asli_gan_ratio,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN + Ratio)\",\n",
        "    sort_by_count=False,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nValidation Set Distribution:\")\n",
        "plot_class_distribution(\n",
        "    val_dataset_asli_gan_ratio,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN + Ratio)\",\n",
        "    sort_by_count=False,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nTest Set Distribution:\")\n",
        "plot_class_distribution(\n",
        "    test_dataset_asli_gan_ratio,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN + Ratio)\",\n",
        "    sort_by_count=False,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "# Plot sorted distributions\n",
        "print(\"\\nTraining Set Distribution (Sorted):\")\n",
        "plot_class_distribution(\n",
        "    train_dataset_asli_gan_ratio,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN + Ratio)\",\n",
        "    sort_by_count=True,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nValidation Set Distribution (Sorted):\")\n",
        "plot_class_distribution(\n",
        "    val_dataset_asli_gan_ratio,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN + Ratio)\",\n",
        "    sort_by_count=True,\n",
        "    show_percentage=True\n",
        ")\n",
        "\n",
        "print(\"\\nTest Set Distribution (Sorted):\")\n",
        "plot_class_distribution(\n",
        "    test_dataset_asli_gan_ratio,\n",
        "    title=\"Image Distribution per Class (Data Asli + GAN + Ratio)\",\n",
        "    sort_by_count=True,\n",
        "    show_percentage=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:08:35.922288Z",
          "iopub.status.busy": "2025-06-07T08:08:35.922005Z",
          "iopub.status.idle": "2025-06-07T08:08:51.647013Z",
          "shell.execute_reply": "2025-06-07T08:08:51.646241Z",
          "shell.execute_reply.started": "2025-06-07T08:08:35.922263Z"
        },
        "id": "1X3S4GlqHGrS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_samples_by_split(dataset, n_samples=5, title=None):\n",
        "    \"\"\"\n",
        "    Plot sample images from each class in a dataset split.\n",
        "\n",
        "    Args:\n",
        "        dataset (WasteDataset): The dataset to sample from\n",
        "        n_samples (int): Number of samples to show per class\n",
        "        title (str): Optional title for the plot\n",
        "    \"\"\"\n",
        "    sampled_images = defaultdict(list)\n",
        "\n",
        "    # Sample images for each class\n",
        "    for label in range(len(dataset.CLASS_NAMES)):\n",
        "        # Get indices for current class\n",
        "        class_indices = [i for i, l in enumerate(dataset.labels) if l == label]\n",
        "        # Randomly sample n_samples images\n",
        "        if len(class_indices) > 0:\n",
        "            sampled_indices = random.sample(class_indices, min(n_samples, len(class_indices)))\n",
        "            sampled_images[label] = sampled_indices\n",
        "\n",
        "    # Create the plot\n",
        "    nrows = len(dataset.CLASS_NAMES)\n",
        "    fig, axs = plt.subplots(nrows, n_samples + 1, figsize=(n_samples * 2.5, nrows * 2))\n",
        "\n",
        "    if title is None:\n",
        "        title = f\"Sample Images - {dataset.split.capitalize()} Split\"\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Plot images for each class\n",
        "    for i, (label, indices) in enumerate(sampled_images.items()):\n",
        "        class_name = dataset.CLASS_NAMES[label]\n",
        "        # Add class name\n",
        "        axs[i, 0].text(0.5, 0.5, class_name, fontsize=12, ha=\"center\", va=\"center\")\n",
        "        axs[i, 0].axis('off')\n",
        "\n",
        "        # Plot sample images\n",
        "        for j, idx in enumerate(indices):\n",
        "            ax = axs[i, j + 1]\n",
        "            try:\n",
        "                img_path = dataset.data[idx]\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                ax.imshow(img)\n",
        "                ax.axis('off')\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "                ax.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "# Plot samples from each split\n",
        "print('---------------------------Data Asli---------------------------')\n",
        "print(\"Training Set Samples:\")\n",
        "plot_samples_by_split(train_dataset_asli, n_samples=5, title=\"Training Set Samples (Data Asli)\")\n",
        "\n",
        "print(\"\\nValidation Set Samples:\")\n",
        "plot_samples_by_split(val_dataset_asli, n_samples=5, title=\"Validation Set Samples (Data Asli)\")\n",
        "\n",
        "print(\"\\nTest Set Samples:\")\n",
        "plot_samples_by_split(test_dataset_asli, n_samples=5, title=\"Test Set Samples (Data Asli)\")\n",
        "print('---------------------------Data Asli + GAN---------------------------')\n",
        "plot_samples_by_split(train_dataset_asli_gan, n_samples=5, title=\"Training Set Samples (Data Asli + GAN)\")\n",
        "\n",
        "print(\"\\nValidation Set Samples:\")\n",
        "plot_samples_by_split(val_dataset_asli_gan, n_samples=5, title=\"Validation Set Samples (Data Asli + GAN)\")\n",
        "\n",
        "print(\"\\nTest Set Samples:\")\n",
        "plot_samples_by_split(test_dataset_asli_gan, n_samples=5, title=\"Test Set Samples (Data Asli + GAN)\")\n",
        "print('---------------------------Data Asli + GAN + Ratio---------------------------')\n",
        "plot_samples_by_split(train_dataset_asli_gan, n_samples=5, title=\"Training Set Samples (Data Asli + GAN + Ratio)\")\n",
        "\n",
        "print(\"\\nValidation Set Samples:\")\n",
        "plot_samples_by_split(val_dataset_asli_gan, n_samples=5, title=\"Validation Set Samples (Data Asli + GAN + Ratio)\")\n",
        "\n",
        "print(\"\\nTest Set Samples:\")\n",
        "plot_samples_by_split(test_dataset_asli_gan, n_samples=5, title=\"Test Set Samples (Data Asli + GAN + Ratio)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8OdA_QCHGrS"
      },
      "source": [
        "# Image Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVjIroWhHGrS"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1dXKrupHGrS"
      },
      "source": [
        "# EFFICIENT NET V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-05T04:04:58.482382Z",
          "iopub.status.busy": "2025-06-05T04:04:58.482114Z",
          "iopub.status.idle": "2025-06-05T04:04:58.486767Z",
          "shell.execute_reply": "2025-06-05T04:04:58.48593Z",
          "shell.execute_reply.started": "2025-06-05T04:04:58.482359Z"
        },
        "id": "xBjJF95bHGrT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "# import timm\n",
        "# from tqdm.auto import tqdm\n",
        "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# # Device Configuration\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "# # Class names for L1, L2, L3\n",
        "# class_names = ['L1', 'L2', 'L3']\n",
        "# num_classes = len(class_names)\n",
        "\n",
        "# def compute_class_weights(labels, num_classes):\n",
        "#     class_counts = np.bincount(labels, minlength=num_classes)\n",
        "#     weights = 1.0 / class_counts\n",
        "#     weights = weights / weights.sum() * num_classes\n",
        "#     return torch.tensor(weights, dtype=torch.float).to(device)\n",
        "\n",
        "# def plot_metrics(history):\n",
        "#     epochs = range(1, len(history['train_loss']) + 1)\n",
        "#     plt.figure(figsize=(12, 5))\n",
        "\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.plot(epochs, history['train_loss'], label=\"Train\")\n",
        "#     plt.plot(epochs, history['val_loss'], label=\"Validation\")\n",
        "#     plt.title(\"Loss\")\n",
        "#     plt.xlabel(\"Epoch\")\n",
        "#     plt.ylabel(\"Loss\")\n",
        "#     plt.legend()\n",
        "\n",
        "#     plt.subplot(1, 2, 2)\n",
        "#     plt.plot(epochs, history['train_acc'], label=\"Train\")\n",
        "#     plt.plot(epochs, history['val_acc'], label=\"Validation\")\n",
        "#     plt.title(\"Accuracy\")\n",
        "#     plt.xlabel(\"Epoch\")\n",
        "#     plt.ylabel(\"Accuracy (%)\")\n",
        "#     plt.legend()\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-05T04:04:58.487817Z",
          "iopub.status.busy": "2025-06-05T04:04:58.487599Z",
          "iopub.status.idle": "2025-06-05T04:04:58.632967Z",
          "shell.execute_reply": "2025-06-05T04:04:58.632197Z",
          "shell.execute_reply.started": "2025-06-05T04:04:58.487799Z"
        },
        "id": "9URRlvdPHGrT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# class EarlyStopping:\n",
        "#     def __init__(self, patience=10, verbose=False):\n",
        "#         self.patience = patience\n",
        "#         self.verbose = verbose\n",
        "#         self.counter = 0\n",
        "#         self.best_loss = None\n",
        "#         self.early_stop = False\n",
        "\n",
        "#     def __call__(self, val_loss):\n",
        "#         if self.best_loss is None:\n",
        "#             self.best_loss = val_loss\n",
        "#         elif val_loss >= self.best_loss:\n",
        "#             self.counter += 1\n",
        "#             if self.verbose:\n",
        "#                 print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
        "#             if self.counter >= self.patience:\n",
        "#                 self.early_stop = True\n",
        "#         else:\n",
        "#             self.best_loss = val_loss\n",
        "#             self.counter = 0\n",
        "\n",
        "# # early_stopping = EarlyStopping(patience=EARLY_STOPPING_PATIENCE, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-05T04:04:58.634074Z",
          "iopub.status.busy": "2025-06-05T04:04:58.633829Z",
          "iopub.status.idle": "2025-06-05T04:04:58.646253Z",
          "shell.execute_reply": "2025-06-05T04:04:58.645587Z",
          "shell.execute_reply.started": "2025-06-05T04:04:58.634053Z"
        },
        "id": "TyH0_rW6HGrT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def evaluate_model(model, test_loader):\n",
        "#     model.eval()\n",
        "#     all_preds = []\n",
        "#     all_labels = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             outputs = model(images)\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "#             all_preds.extend(predicted.cpu().numpy())\n",
        "#             all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "#     return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "# def plot_confusion_matrix(y_true, y_pred, class_names):\n",
        "#     # Plot normalized confusion matrix\n",
        "#     cm_norm = confusion_matrix(y_true, y_pred, normalize='true') * 100\n",
        "#     plt.figure(figsize=(8, 6))\n",
        "#     sns.heatmap(cm_norm, annot=True, fmt='.1f', cmap='Blues',\n",
        "#                 xticklabels=class_names, yticklabels=class_names)\n",
        "#     plt.title(\"Normalized Confusion Matrix (%)\")\n",
        "#     plt.xlabel(\"Predicted\")\n",
        "#     plt.ylabel(\"True\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "#     # Plot confusion matrix with actual counts\n",
        "#     cm = confusion_matrix(y_true, y_pred)\n",
        "#     plt.figure(figsize=(10, 8))\n",
        "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "#                 xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "#     # Add row and column totals\n",
        "#     row_sums = cm.sum(axis=1)\n",
        "#     col_sums = cm.sum(axis=0)\n",
        "\n",
        "#     # Add row totals\n",
        "#     for i in range(len(class_names)):\n",
        "#         plt.text(len(class_names) + 0.5, i + 0.5, f'Total: {row_sums[i]}',\n",
        "#                 ha='left', va='center')\n",
        "\n",
        "#     # Add column totals\n",
        "#     for i in range(len(class_names)):\n",
        "#         plt.text(i + 0.5, len(class_names) + 0.5, f'Total: {col_sums[i]}',\n",
        "#                 ha='center', va='bottom')\n",
        "\n",
        "#     plt.title(\"Confusion Matrix (Counts)\")\n",
        "#     plt.xlabel(\"Predicted\")\n",
        "#     plt.ylabel(\"True\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# def plot_classification_metrics(y_true, y_pred, class_names):\n",
        "#     report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "#     metrics = ['precision', 'recall', 'f1-score']\n",
        "\n",
        "#     # Prepare data for plotting\n",
        "#     class_metrics = {metric: [] for metric in metrics}\n",
        "#     for class_name in class_names:\n",
        "#         for metric in metrics:\n",
        "#             class_metrics[metric].append(report[class_name][metric])\n",
        "\n",
        "#     # Plot metrics\n",
        "#     fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "#     for ax, metric in zip(axes, metrics):\n",
        "#         sns.barplot(x=class_metrics[metric], y=class_names, ax=ax, palette=\"viridis\")\n",
        "#         ax.set_title(f\"{metric.capitalize()}\")\n",
        "#         ax.set_xlim(0, 1)\n",
        "#         for i, v in enumerate(class_metrics[metric]):\n",
        "#             ax.text(v + 0.01, i, f\"{v:.2%}\", va=\"center\")\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "#     # Print detailed report\n",
        "#     print(\"\\nClassification Report:\")\n",
        "#     print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "#     # Print class distribution\n",
        "#     print(\"\\nClass Distribution and Metrics:\")\n",
        "#     for i, class_name in enumerate(class_names):\n",
        "#         true_count = sum(y_true == i)\n",
        "#         pred_count = sum(y_pred == i)\n",
        "#         tp = sum((y_true == i) & (y_pred == i))\n",
        "#         recall = tp / true_count * 100 if true_count > 0 else 0\n",
        "#         precision = tp / pred_count * 100 if pred_count > 0 else 0\n",
        "#         print(f\"{class_name}:\")\n",
        "#         print(f\"  True samples: {true_count}\")\n",
        "#         print(f\"  Predicted samples: {pred_count}\")\n",
        "#         print(f\"  True Positives: {tp}\")\n",
        "#         print(f\"  Recall: {recall:.2f}%\")\n",
        "#         print(f\"  Precision: {precision:.2f}%\")\n",
        "#         print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-05T04:04:58.647173Z",
          "iopub.status.busy": "2025-06-05T04:04:58.64691Z",
          "iopub.status.idle": "2025-06-05T04:04:58.660328Z",
          "shell.execute_reply": "2025-06-05T04:04:58.659636Z",
          "shell.execute_reply.started": "2025-06-05T04:04:58.647134Z"
        },
        "id": "5Jt60zGMHGrT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def train_model(\n",
        "#     model,\n",
        "#     train_loader,\n",
        "#     val_loader,\n",
        "#     early_stopping,\n",
        "#     lr=0.001,\n",
        "#     weight_decay=1e-4,\n",
        "#     best_model_path=\"best_model.pth\",\n",
        "#     max_epochs=100,  # Batas maksimum epoch\n",
        "#     val_loss_threshold=0.5  # Stop jika val loss < 0.5\n",
        "# ):\n",
        "#     model.to(device)\n",
        "#     history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "#     # Compute class weights\n",
        "#     class_weights = compute_class_weights(train_loader.dataset.labels, num_classes)\n",
        "#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "#     scheduler = ReduceLROnPlateau(optimizer, patience=3, factor=0.5, verbose=True)\n",
        "\n",
        "#     best_val_acc = 0\n",
        "#     epoch = 0\n",
        "\n",
        "#     best_val_loss = float(\"inf\")  # Tambahkan ini di awal sebelum while loop\n",
        "\n",
        "#     while epoch < max_epochs:\n",
        "#         epoch += 1\n",
        "\n",
        "#         # Training phase\n",
        "#         model.train()\n",
        "#         train_loss, train_correct, total = 0, 0, 0\n",
        "#         for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\"):\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(images)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             train_loss += loss.item()\n",
        "#             preds = outputs.argmax(1)\n",
        "#             train_correct += (preds == labels).sum().item()\n",
        "#             total += labels.size(0)\n",
        "\n",
        "#         # Validation phase\n",
        "#         model.eval()\n",
        "#         val_loss, val_correct = 0, 0\n",
        "#         with torch.no_grad():\n",
        "#             for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
        "#                 images, labels = images.to(device), labels.to(device)\n",
        "#                 outputs = model(images)\n",
        "#                 loss = criterion(outputs, labels)\n",
        "\n",
        "#                 val_loss += loss.item()\n",
        "#                 preds = outputs.argmax(1)\n",
        "#                 val_correct += (preds == labels).sum().item()\n",
        "\n",
        "#         # Calculate metrics\n",
        "#         epoch_train_loss = train_loss / len(train_loader)\n",
        "#         epoch_val_loss = val_loss / len(val_loader)\n",
        "#         epoch_train_acc = 100 * train_correct / total\n",
        "#         epoch_val_acc = 100 * val_correct / len(val_loader.dataset)\n",
        "\n",
        "#         # Update learning rate\n",
        "#         scheduler.step(epoch_val_loss)\n",
        "\n",
        "#         # Save metrics\n",
        "#         history['train_loss'].append(epoch_train_loss)\n",
        "#         history['val_loss'].append(epoch_val_loss)\n",
        "#         history['train_acc'].append(epoch_train_acc)\n",
        "#         history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "#         print(f\"Epoch {epoch}\")\n",
        "#         print(f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%\")\n",
        "#         print(f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "#         # Save best model\n",
        "#         # Based on Accuracy\n",
        "#         # if epoch_val_acc > best_val_acc:\n",
        "#         #     best_val_acc = epoch_val_acc\n",
        "#         #     torch.save(model.state_dict(), best_model_path)\n",
        "#         #     print(f\"New best model saved! (Val Acc: {best_val_acc:.2f}%)\")\n",
        "\n",
        "#         # Based on Loss\n",
        "#         if epoch_val_loss < best_val_loss:\n",
        "#             best_val_loss = epoch_val_loss\n",
        "#             torch.save(model.state_dict(), best_model_path)\n",
        "#             print(f\"New best model saved! (Val Loss: {best_val_loss:.4f})\")\n",
        "\n",
        "#         # # Stop if val loss is below threshold\n",
        "#         # if epoch_val_loss < val_loss_threshold:\n",
        "#         #     print(f\"Validation loss < {val_loss_threshold}. Stopping training.\")\n",
        "#         #     break\n",
        "\n",
        "#         # Early stopping check\n",
        "#         early_stopping(epoch_val_loss)\n",
        "#         if early_stopping.early_stop:\n",
        "#             print(\"Early stopping triggered.\")\n",
        "#             break\n",
        "\n",
        "#     return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTpmCLS2HGrT"
      },
      "source": [
        "#### Data Asli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-05T04:04:58.661369Z",
          "iopub.status.busy": "2025-06-05T04:04:58.661086Z",
          "iopub.status.idle": "2025-06-05T04:04:58.675278Z",
          "shell.execute_reply": "2025-06-05T04:04:58.674574Z",
          "shell.execute_reply.started": "2025-06-05T04:04:58.661343Z"
        },
        "id": "cBoBdMMyHGrU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# print('---------------------------Data Asli---------------------------')\n",
        "# # Initialize a NEW model for Data Asli\n",
        "# model_asli = timm.create_model('resnet50', pretrained=False)\n",
        "# model_asli.fc = nn.Linear(model_asli.fc.in_features, num_classes)\n",
        "\n",
        "# early_stopping_asli = EarlyStopping(patience=15, verbose=True)\n",
        "# model_asli, history_asli = train_model(\n",
        "#     model=model_asli, # Use the new model_asli\n",
        "#     train_loader=train_loader_asli,\n",
        "#     val_loader=val_loader_asli,\n",
        "#     early_stopping=early_stopping_asli,\n",
        "#     weight_decay=1e-4,\n",
        "#     best_model_path=\"best_model_asli.pth\"\n",
        "# )\n",
        "\n",
        "# # Plot training history for Data Asli\n",
        "# plot_metrics(history_asli)\n",
        "\n",
        "# # Evaluate on test set for Data Asli\n",
        "# y_pred_asli, y_true_asli = evaluate_model(model_asli, test_loader_asli)\n",
        "\n",
        "# # Plot confusion matrix and classification metrics for Data Asli\n",
        "# plot_confusion_matrix(y_true_asli, y_pred_asli, class_names)\n",
        "# plot_classification_metrics(y_true_asli, y_pred_asli, class_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrXWqw3sHGrU"
      },
      "source": [
        "#### Model Data Asli + GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-05T04:04:58.676121Z",
          "iopub.status.busy": "2025-06-05T04:04:58.675933Z",
          "iopub.status.idle": "2025-06-05T04:04:58.688737Z",
          "shell.execute_reply": "2025-06-05T04:04:58.688175Z",
          "shell.execute_reply.started": "2025-06-05T04:04:58.676107Z"
        },
        "id": "eMyES2IZHGrU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# print('---------------------------Data Asli + GAN---------------------------')\n",
        "# # Initialize a NEW model for Data Asli + GAN\n",
        "# model_asli_gan = timm.create_model('resnet50', pretrained=False)\n",
        "# model_asli_gan.fc = nn.Linear(model_asli_gan.fc.in_features, num_classes)\n",
        "\n",
        "# early_stopping_asli_gan = EarlyStopping(patience=15, verbose=True)\n",
        "# model_asli_gan, history_asli_gan = train_model(\n",
        "#     model=model_asli_gan, # Use the new model_asli_gan\n",
        "#     train_loader=train_loader_asli_gan,\n",
        "#     val_loader=val_loader_asli_gan,\n",
        "#     early_stopping=early_stopping_asli_gan,\n",
        "#     weight_decay=1e-4,\n",
        "#     best_model_path=\"best_model_asli_gan.pth\"\n",
        "# )\n",
        "\n",
        "# # Plot training history for Data Asli + GAN\n",
        "# plot_metrics(history_asli_gan)\n",
        "\n",
        "# # Evaluate on test set for Data Asli + GAN\n",
        "# y_pred_asli_gan, y_true_asli_gan = evaluate_model(model_asli_gan, test_loader_asli_gan)\n",
        "\n",
        "# # Plot confusion matrix and classification metrics for Data Asli + GAN\n",
        "# plot_confusion_matrix(y_true_asli_gan, y_pred_asli_gan, class_names)\n",
        "# plot_classification_metrics(y_true_asli_gan, y_pred_asli_gan, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIK_0AptHGrU"
      },
      "source": [
        "#### Model Data Asli Gan Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-05T04:04:58.691502Z",
          "iopub.status.busy": "2025-06-05T04:04:58.691293Z",
          "iopub.status.idle": "2025-06-05T04:04:58.70327Z",
          "shell.execute_reply": "2025-06-05T04:04:58.702443Z",
          "shell.execute_reply.started": "2025-06-05T04:04:58.691489Z"
        },
        "id": "ObvuC6J-HGrU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# print('---------------------------Data Asli + GAN + Ratio---------------------------')\n",
        "# # Initialize a NEW model for Data Asli + GAN + Ratio\n",
        "# model_asli_gan_ratio = timm.create_model('resnet50', pretrained=False)\n",
        "# model_asli_gan_ratio.fc = nn.Linear(model_asli_gan_ratio.fc.in_features, num_classes)\n",
        "\n",
        "# early_stopping_asli_gan_ratio = EarlyStopping(patience=15, verbose=True)\n",
        "\n",
        "# model_asli_gan_ratio, history_asli_gan_ratio = train_model(\n",
        "#     model=model_asli_gan_ratio, # Use the new model_asli_gan_ratio\n",
        "#     train_loader=train_loader_asli_gan_ratio,\n",
        "#     val_loader=val_loader_asli_gan_ratio,\n",
        "#     early_stopping=early_stopping_asli_gan_ratio,\n",
        "#     weight_decay=1e-4,\n",
        "#     best_model_path=\"best_model_asli_gan_ratio.pth\"\n",
        "# )\n",
        "\n",
        "# # Plot training history for Data Asli + GAN + Ratio\n",
        "# plot_metrics(history_asli_gan_ratio)\n",
        "\n",
        "# # Evaluate on test set for Data Asli + GAN + Ratio\n",
        "# y_pred_asli_gan_ratio, y_true_asli_gan_ratio = evaluate_model(model_asli_gan_ratio, test_loader_asli_gan_ratio)\n",
        "\n",
        "# # Plot confusion matrix and classification metrics for Data Asli + GAN + Ratio\n",
        "# plot_confusion_matrix(y_true_asli_gan_ratio, y_pred_asli_gan_ratio, class_names)\n",
        "# plot_classification_metrics(y_true_asli_gan_ratio, y_pred_asli_gan_ratio, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXlCx5UnHGrU"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:08:51.649765Z",
          "iopub.status.busy": "2025-06-07T08:08:51.649348Z",
          "iopub.status.idle": "2025-06-07T08:08:51.789646Z",
          "shell.execute_reply": "2025-06-07T08:08:51.788878Z",
          "shell.execute_reply.started": "2025-06-07T08:08:51.649742Z"
        },
        "id": "SoksDq-bHGrU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import timm\n",
        "from tqdm.auto import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:08:51.79065Z",
          "iopub.status.busy": "2025-06-07T08:08:51.790443Z",
          "iopub.status.idle": "2025-06-07T08:08:51.798883Z",
          "shell.execute_reply": "2025-06-07T08:08:51.798085Z",
          "shell.execute_reply.started": "2025-06-07T08:08:51.790635Z"
        },
        "id": "3eU38-L9HGrV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "\n",
        "# Device Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Class names for L1, L2, L3\n",
        "class_names = ['L1', 'L2', 'L3']\n",
        "num_classes = len(class_names)\n",
        "\n",
        "def compute_class_weights(labels, num_classes):\n",
        "    class_counts = np.bincount(labels, minlength=num_classes)\n",
        "    weights = 1.0 / class_counts\n",
        "    weights = weights / weights.sum() * num_classes\n",
        "    return torch.tensor(weights, dtype=torch.float).to(device)\n",
        "\n",
        "def plot_metrics(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], label=\"Train\")\n",
        "    plt.plot(epochs, history['val_loss'], label=\"Validation\")\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['train_acc'], label=\"Train\")\n",
        "    plt.plot(epochs, history['val_acc'], label=\"Validation\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-05T04:04:58.849967Z",
          "iopub.status.busy": "2025-06-05T04:04:58.849694Z",
          "iopub.status.idle": "2025-06-05T04:04:58.863381Z",
          "shell.execute_reply": "2025-06-05T04:04:58.862678Z",
          "shell.execute_reply.started": "2025-06-05T04:04:58.849941Z"
        },
        "id": "oBj5jzVGHGrV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # OG Early Stopping\n",
        "# class EarlyStopping:\n",
        "#     def __init__(self, patience=10, verbose=False):\n",
        "#         self.patience = patience\n",
        "#         self.verbose = verbose\n",
        "#         self.counter = 0\n",
        "#         self.best_loss = None\n",
        "#         self.early_stop = False\n",
        "\n",
        "#     def __call__(self, val_loss):\n",
        "#         if self.best_loss is None:\n",
        "#             self.best_loss = val_loss\n",
        "#         elif val_loss >= self.best_loss:\n",
        "#             self.counter += 1\n",
        "#             if self.verbose:\n",
        "#                 print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
        "#             if self.counter >= self.patience:\n",
        "#                 self.early_stop = True\n",
        "#         else:\n",
        "#             self.best_loss = val_loss\n",
        "#             self.counter = 0\n",
        "\n",
        "# # early_stopping = EarlyStopping(patience=EARLY_STOPPING_PATIENCE, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:08:51.800026Z",
          "iopub.status.busy": "2025-06-07T08:08:51.799745Z",
          "iopub.status.idle": "2025-06-07T08:08:51.814764Z",
          "shell.execute_reply": "2025-06-07T08:08:51.814092Z",
          "shell.execute_reply.started": "2025-06-07T08:08:51.80001Z"
        },
        "id": "0moOjFIQHGrV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, verbose=False, min_delta=1e-3): # Tambahkan min_delta\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.min_delta = min_delta # Inisialisasi min_delta\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        # Perubahan di sini: val_loss harus kurang dari (self.best_loss - self.min_delta) untuk dianggap sebagai peningkatan\n",
        "        elif val_loss >= self.best_loss - self.min_delta: # Jika tidak ada penurunan yang signifikan\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else: # Ada penurunan yang signifikan\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:08:51.815761Z",
          "iopub.status.busy": "2025-06-07T08:08:51.815533Z",
          "iopub.status.idle": "2025-06-07T08:08:51.833301Z",
          "shell.execute_reply": "2025-06-07T08:08:51.832702Z",
          "shell.execute_reply.started": "2025-06-07T08:08:51.815739Z"
        },
        "id": "xli3bs8AHGrV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# --- Fungsi evaluate_model yang dimodifikasi ---\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    mispredicted_data = [] # List untuk menyimpan data gambar yang salah diprediksi\n",
        "\n",
        "    # Dapatkan akses ke dataset untuk mengambil jalur gambar\n",
        "    # Asumsi test_loader.dataset adalah instance dari kelas Dataset kustom Anda\n",
        "    test_dataset = test_loader.dataset\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
        "            images_on_device, labels_on_device = images.to(device), labels.to(device)\n",
        "            outputs = model(images_on_device)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Iterasi melalui setiap sampel dalam batch\n",
        "            for j in range(len(labels_on_device)):\n",
        "                true_label = labels_on_device[j].item()\n",
        "                pred_label = predicted[j].item()\n",
        "\n",
        "                all_preds.append(pred_label)\n",
        "                all_labels.append(true_label)\n",
        "\n",
        "                # Jika prediksi salah, simpan informasi gambar\n",
        "                if true_label != pred_label:\n",
        "                    # Ambil indeks global sampel di dataset\n",
        "                    # Ini sedikit tricky karena test_loader mengembalikan batch\n",
        "                    # Anda perlu cara untuk memetakan kembali ke indeks asli dataset\n",
        "                    # Cara paling mudah adalah jika Dataset Anda menyimpan path\n",
        "                    # atau jika Anda bisa menghitung indeks global dari batch_idx dan item_idx\n",
        "                    # Untuk kesederhanaan, kita akan mengasumsikan test_dataset.data menyimpan jalur gambar\n",
        "                    # dan menggunakan i * batch_size + j sebagai indeks.\n",
        "                    # PERHATIAN: Ini mungkin tidak 100% akurat jika test_loader tidak mengembalikan item secara berurutan\n",
        "                    # atau jika ada filter di dataset. Pastikan test_loader shuffle=False.\n",
        "\n",
        "                    # Cara yang lebih aman adalah dengan mengembalikan tensor gambar itu sendiri\n",
        "                    # dari evaluate_model, atau menyimpan indeks asli di __getitem__ Dataset.\n",
        "                    # Untuk saat ini, mari kita asumsikan test_dataset.data bisa diakses seperti ini:\n",
        "                    original_image_path = test_dataset.data[i * test_loader.batch_size + j]\n",
        "\n",
        "                    mispredicted_data.append({\n",
        "                        'image_path': original_image_path,\n",
        "                        'true_label': true_label,\n",
        "                        'predicted_label': pred_label\n",
        "                    })\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels), mispredicted_data\n",
        "\n",
        "# --- Fungsi baru untuk plotting gambar yang salah diprediksi ---\n",
        "def plot_mispredicted_images(mispredicted_data, class_names, num_images_to_show=10):\n",
        "    if not mispredicted_data:\n",
        "        print(\"No mispredicted images to show.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- Showing {min(num_images_to_show, len(mispredicted_data))} Mispredicted Images ---\")\n",
        "\n",
        "    # Ambil beberapa sampel acak dari gambar yang salah diprediksi\n",
        "    display_data = random.sample(mispredicted_data, min(num_images_to_show, len(mispredicted_data)))\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(display_data), figsize=(len(display_data) * 3, 4))\n",
        "    if len(display_data) == 1:\n",
        "        axes = [axes] # Pastikan axes selalu iterable\n",
        "\n",
        "    for i, data in enumerate(display_data):\n",
        "        img_path = data['image_path']\n",
        "        true_label = data['true_label']\n",
        "        predicted_label = data['predicted_label']\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            axes[i].imshow(img)\n",
        "            axes[i].set_title(\n",
        "                f\"True: {class_names[true_label]}\\nPred: {class_names[predicted_label]}\",\n",
        "                color='red' # Untuk menandakan kesalahan\n",
        "            )\n",
        "            axes[i].axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path} for plotting: {e}\")\n",
        "            axes[i].set_title(\"Error loading image\")\n",
        "            axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- Fungsi plot_confusion_matrix dan plot_classification_metrics tetap sama ---\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names): #\n",
        "    # Plot normalized confusion matrix\n",
        "    cm_norm = confusion_matrix(y_true, y_pred, normalize='true') * 100\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_norm, annot=True, fmt='.1f', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(\"Normalized Confusion Matrix (%)\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot confusion matrix with actual counts\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "    # Add row and column totals\n",
        "    row_sums = cm.sum(axis=1)\n",
        "    col_sums = cm.sum(axis=0)\n",
        "\n",
        "    # Add row totals\n",
        "    for i in range(len(class_names)):\n",
        "        plt.text(len(class_names) + 0.5, i + 0.5, f'Total: {row_sums[i]}',\n",
        "                ha='left', va='center')\n",
        "\n",
        "    # Add column totals\n",
        "    for i in range(len(class_names)):\n",
        "        plt.text(i + 0.5, len(class_names) + 0.5, f'Total: {col_sums[i]}',\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "    plt.title(\"Confusion Matrix (Counts)\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_classification_metrics(y_true, y_pred, class_names): #\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "    metrics = ['precision', 'recall', 'f1-score']\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    class_metrics = {metric: [] for metric in metrics}\n",
        "    for class_name in class_names:\n",
        "        for metric in metrics:\n",
        "            class_metrics[metric].append(report[class_name][metric])\n",
        "\n",
        "    # Plot metrics\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    for ax, metric in zip(axes, metrics):\n",
        "        sns.barplot(x=class_metrics[metric], y=class_names, ax=ax, palette=\"viridis\")\n",
        "        ax.set_title(f\"{metric.capitalize()}\")\n",
        "        ax.set_xlim(0, 1)\n",
        "        for i, v in enumerate(class_metrics[metric]):\n",
        "            ax.text(v + 0.01, i, f\"{v:.2%}\", va=\"center\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "    # Print class distribution\n",
        "    print(\"\\nClass Distribution and Metrics:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        true_count = sum(y_true == i)\n",
        "        pred_count = sum(y_pred == i)\n",
        "        tp = sum((y_true == i) & (y_pred == i))\n",
        "        recall = tp / true_count * 100 if true_count > 0 else 0\n",
        "        precision = tp / pred_count * 100 if pred_count > 0 else 0\n",
        "        print(f\"{class_name}:\")\n",
        "        print(f\"  True samples: {true_count}\")\n",
        "        print(f\"  Predicted samples: {pred_count}\")\n",
        "        print(f\"  True Positives: {tp}\")\n",
        "        print(f\"  Recall: {recall:.2f}%\")\n",
        "        print(f\"  Precision: {precision:.2f}%\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-05T04:04:59.208578Z",
          "iopub.status.busy": "2025-06-05T04:04:59.208312Z",
          "iopub.status.idle": "2025-06-05T04:04:59.223057Z",
          "shell.execute_reply": "2025-06-05T04:04:59.222497Z",
          "shell.execute_reply.started": "2025-06-05T04:04:59.208555Z"
        },
        "id": "q388OuScHGrW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ### ORIGINAL\n",
        "# def train_model(\n",
        "#     model,\n",
        "#     train_loader,\n",
        "#     val_loader,\n",
        "#     early_stopping,\n",
        "#     lr=0.001,\n",
        "#     weight_decay=1e-4,\n",
        "#     best_model_path=\"best_model.pth\",\n",
        "#     max_epochs=100,  # Batas maksimum epoch\n",
        "#     val_loss_threshold=0.5  # Stop jika val loss < 0.5\n",
        "# ):\n",
        "#     model.to(device)\n",
        "#     history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "#     # Compute class weights\n",
        "#     class_weights = compute_class_weights(train_loader.dataset.labels, num_classes)\n",
        "#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "#     scheduler = ReduceLROnPlateau(optimizer, patience=3, factor=0.5, verbose=True)\n",
        "\n",
        "#     best_val_acc = 0\n",
        "#     epoch = 0\n",
        "\n",
        "#     best_val_loss = float(\"inf\")  # Tambahkan ini di awal sebelum while loop\n",
        "\n",
        "#     while epoch < max_epochs:\n",
        "#         epoch += 1\n",
        "\n",
        "#         # Training phase\n",
        "#         model.train()\n",
        "#         train_loss, train_correct, total = 0, 0, 0\n",
        "#         for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\"):\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(images)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             train_loss += loss.item()\n",
        "#             preds = outputs.argmax(1)\n",
        "#             train_correct += (preds == labels).sum().item()\n",
        "#             total += labels.size(0)\n",
        "\n",
        "#         # Validation phase\n",
        "#         model.eval()\n",
        "#         val_loss, val_correct = 0, 0\n",
        "#         with torch.no_grad():\n",
        "#             for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
        "#                 images, labels = images.to(device), labels.to(device)\n",
        "#                 outputs = model(images)\n",
        "#                 loss = criterion(outputs, labels)\n",
        "\n",
        "#                 val_loss += loss.item()\n",
        "#                 preds = outputs.argmax(1)\n",
        "#                 val_correct += (preds == labels).sum().item()\n",
        "\n",
        "#         # Calculate metrics\n",
        "#         epoch_train_loss = train_loss / len(train_loader)\n",
        "#         epoch_val_loss = val_loss / len(val_loader)\n",
        "#         epoch_train_acc = 100 * train_correct / total\n",
        "#         epoch_val_acc = 100 * val_correct / len(val_loader.dataset)\n",
        "\n",
        "#         # Update learning rate\n",
        "#         scheduler.step(epoch_val_loss)\n",
        "\n",
        "#         # Save metrics\n",
        "#         history['train_loss'].append(epoch_train_loss)\n",
        "#         history['val_loss'].append(epoch_val_loss)\n",
        "#         history['train_acc'].append(epoch_train_acc)\n",
        "#         history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "#         print(f\"Epoch {epoch}\")\n",
        "#         print(f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%\")\n",
        "#         print(f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "#         # Save best model\n",
        "#         # Based on Accuracy\n",
        "#         # if epoch_val_acc > best_val_acc:\n",
        "#         #     best_val_acc = epoch_val_acc\n",
        "#         #     torch.save(model.state_dict(), best_model_path)\n",
        "#         #     print(f\"New best model saved! (Val Acc: {best_val_acc:.2f}%)\")\n",
        "\n",
        "#         # Based on Loss\n",
        "#         if epoch_val_loss < best_val_loss:\n",
        "#             best_val_loss = epoch_val_loss\n",
        "#             torch.save(model.state_dict(), best_model_path)\n",
        "#             print(f\"New best model saved! (Val Loss: {best_val_loss:.4f})\")\n",
        "\n",
        "#         # # Stop if val loss is below threshold\n",
        "#         # if epoch_val_loss < val_loss_threshold:\n",
        "#         #     print(f\"Validation loss < {val_loss_threshold}. Stopping training.\")\n",
        "#         #     break\n",
        "\n",
        "#         # Early stopping check\n",
        "#         early_stopping(epoch_val_loss)\n",
        "#         if early_stopping.early_stop:\n",
        "#             print(\"Early stopping triggered.\")\n",
        "#             break\n",
        "\n",
        "#     return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:08:51.834421Z",
          "iopub.status.busy": "2025-06-07T08:08:51.83415Z",
          "iopub.status.idle": "2025-06-07T08:08:51.849467Z",
          "shell.execute_reply": "2025-06-07T08:08:51.848745Z",
          "shell.execute_reply.started": "2025-06-07T08:08:51.834397Z"
        },
        "id": "DyctbEAXHGrW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    early_stopping=None,\n",
        "    lr=0.001,\n",
        "    weight_decay=1e-4,\n",
        "    best_model_path=\"best_model.pth\",\n",
        "    max_epochs=100,\n",
        "    val_loss_threshold=0.5\n",
        "):\n",
        "    model.to(device)\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "    class_weights = compute_class_weights(train_loader.dataset.labels, num_classes)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, patience=3, factor=0.5, verbose=True)\n",
        "\n",
        "    # Inisialisasi best_val_loss hanya jika early_stopping diaktifkan\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    # Tambahkan variabel untuk menyimpan state_dict model terakhir jika early_stopping=None\n",
        "    last_model_state = None\n",
        "\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch < max_epochs:\n",
        "        epoch += 1\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, train_correct, total = 0, 0, 0\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            preds = outputs.argmax(1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, val_correct = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                preds = outputs.argmax(1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        epoch_train_loss = train_loss / len(train_loader)\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        epoch_train_acc = 100 * train_correct / total\n",
        "        epoch_val_acc = 100 * val_correct / len(val_loader.dataset)\n",
        "\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch}\")\n",
        "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%\\n\"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%\")\n",
        "\n",
        "        # --- Logika Penyimpanan Model yang Dimodifikasi ---\n",
        "        if early_stopping is not None:\n",
        "            # Jika early_stopping aktif, save model terbaik berdasarkan validasi loss\n",
        "            if epoch_val_loss < best_val_loss:\n",
        "                best_val_loss = epoch_val_loss\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                print(f\"New best model saved! (Val Loss: {best_val_loss:.4f})\")\n",
        "\n",
        "            # Cek early stopping\n",
        "            early_stopping(epoch_val_loss)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "        else:\n",
        "            # Jika early_stopping dinonaktifkan (None), update state_dict model terakhir\n",
        "            last_model_state = model.state_dict()\n",
        "            # Di sini kita tidak save ke disk di setiap epoch, hanya di akhir loop\n",
        "            # Proses save ke disk akan dilakukan di luar loop ini\n",
        "\n",
        "    # --- Simpan model terakhir ke disk jika early_stopping dinonaktifkan ---\n",
        "    if early_stopping is None and last_model_state is not None:\n",
        "        torch.save(last_model_state, best_model_path)\n",
        "        print(f\"Model from last epoch ({max_epochs}) saved to {best_model_path}\")\n",
        "        # Penting: Setelah menyimpan state_dict terakhir, pastikan model dimuat kembali dengan state_dict ini\n",
        "        # untuk memastikan model yang dikembalikan adalah model terakhir, bukan yang terbaik dari epoch sebelumnya\n",
        "        model.load_state_dict(last_model_state)\n",
        "    elif early_stopping is not None: # Jika early_stopping aktif dan model terbaik sudah disimpan\n",
        "        # Muat kembali model terbaik yang sudah disimpan jika early stopping aktif dan sudah ada model disimpan\n",
        "        if os.path.exists(best_model_path):\n",
        "            model.load_state_dict(torch.load(best_model_path))\n",
        "            print(f\"Loaded best model from {best_model_path}\")\n",
        "        else:\n",
        "            print(f\"Warning: Best model not found at {best_model_path}. Returning current model state.\")\n",
        "\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T07:41:50.018322Z",
          "iopub.status.busy": "2025-06-07T07:41:50.017644Z",
          "iopub.status.idle": "2025-06-07T07:41:50.031158Z",
          "shell.execute_reply": "2025-06-07T07:41:50.030626Z",
          "shell.execute_reply.started": "2025-06-07T07:41:50.018299Z"
        },
        "id": "fbgcjGPMHGrX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# class SimpleCNN(nn.Module):\n",
        "#     def __init__(self, num_classes=3):\n",
        "#         super(SimpleCNN, self).__init__()\n",
        "#         # Layer konvolusi pertama\n",
        "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "#         self.bn1 = nn.BatchNorm2d(16)\n",
        "#         # self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # Mengurangi ukuran gambar / 2\n",
        "#         self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2) # Mengurangi ukuran gambar / 2\n",
        "\n",
        "#         # Layer konvolusi kedua\n",
        "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(32)\n",
        "#         # self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # Mengurangi ukuran gambar / 2\n",
        "#         self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2) # Mengurangi ukuran gambar / 2\n",
        "\n",
        "#         # Layer konvolusi ketiga\n",
        "#         self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "#         self.bn3 = nn.BatchNorm2d(64)\n",
        "#         # self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # Mengurangi ukuran gambar / 2\n",
        "#         self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2) # Mengurangi ukuran gambar / 2\n",
        "\n",
        "#         # --- PENTING: Hitung ukuran input untuk layer FC berdasarkan IMAGE_SIZE asli Anda ---\n",
        "#         # Untuk input 256x256 piksel, setelah 3 kali MaxPool2d dengan kernel_size=2, stride=2:\n",
        "#         # 256 / 2 = 128\n",
        "#         # 128 / 2 = 64\n",
        "#         # 64 / 2 = 32\n",
        "#         # Jadi, output dari conv layer terakhir adalah 64 channel dengan ukuran 32x32\n",
        "#         self.fc_input_features = 64 * 32 * 32 # 64 channels * 32 * 32 = 65536\n",
        "#         self.fc = nn.Linear(self.fc_input_features, num_classes)\n",
        "#         self.dropout = nn.Dropout(0.5) # Tambahkan Dropout dengan probabilitas 0.5\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "#         x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "#         x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
        "#         # x = self.pool1(F.relu(self.conv1(x)))\n",
        "#         # x = self.pool2(F.relu(self.conv2(x)))\n",
        "#         # x = self.pool3(F.relu(self.conv3(x)))\n",
        "\n",
        "#         # Flatten the tensor for the fully connected layer\n",
        "#         x = x.view(-1, self.fc_input_features)\n",
        "#         x = self.dropout(x) # Terapkan Dropout sebelum layer FC\n",
        "#         x = self.fc(x)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:08:51.850385Z",
          "iopub.status.busy": "2025-06-07T08:08:51.850144Z",
          "iopub.status.idle": "2025-06-07T08:08:51.862532Z",
          "shell.execute_reply": "2025-06-07T08:08:51.861868Z",
          "shell.execute_reply.started": "2025-06-07T08:08:51.850371Z"
        },
        "id": "GRYjXcGrHGrX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch # Pastikan torch diimpor untuk torch.cat\n",
        "\n",
        "# class SimpleCNN(nn.Module):\n",
        "#     def __init__(self, num_classes=3):\n",
        "#         super(SimpleCNN, self).__init__()\n",
        "\n",
        "#         # Conv Block 1\n",
        "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "#         self.bn1 = nn.BatchNorm2d(16)\n",
        "#         self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "#         self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # Conv Block 2 (in_channels sekarang 16 * 2 = 32)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=16 * 2, out_channels=32, kernel_size=3, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(32)\n",
        "#         self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "#         self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # Conv Block 3 (in_channels sekarang 32 * 2 = 64)\n",
        "#         self.conv3 = nn.Conv2d(in_channels=32 * 2, out_channels=64, kernel_size=3, padding=1)\n",
        "#         self.bn3 = nn.BatchNorm2d(64)\n",
        "#         self.avgpool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "#         self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # Hitung ukuran input untuk layer FC\n",
        "#         # Output channels dari conv3 adalah 64. Setelah pooling ganda dan concatenate, jadi 64 * 2 = 128 channels.\n",
        "#         # Ukuran spasial (256x256 -> 128x128 -> 64x64 -> 32x32)\n",
        "#         self.fc_input_features = (64 * 2) * 32 * 32 # 128 channels * 32 * 32 = 131072\n",
        "#         self.fc = nn.Linear(self.fc_input_features, num_classes)\n",
        "#         self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Blok 1: Conv -> BatchNorm -> ReLU -> (AvgPool, MaxPool) -> Concat\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.bn1(x)\n",
        "#         x = F.relu(x)\n",
        "#         x_avg = self.avgpool1(x)\n",
        "#         x_max = self.maxpool1(x)\n",
        "#         x = torch.cat([x_avg, x_max], dim=1) # Concatenate output AvgPool dan MaxPool\n",
        "\n",
        "#         # Blok 2: Conv (input dari concat sebelumnya) -> BatchNorm -> ReLU -> (AvgPool, MaxPool) -> Concat\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.bn2(x)\n",
        "#         x = F.relu(x)\n",
        "#         x_avg = self.avgpool2(x)\n",
        "#         x_max = self.maxpool2(x)\n",
        "#         x = torch.cat([x_avg, x_max], dim=1) # Concatenate output AvgPool dan MaxPool\n",
        "\n",
        "#         # Blok 3: Conv (input dari concat sebelumnya) -> BatchNorm -> ReLU -> (AvgPool, MaxPool) -> Concat\n",
        "#         x = self.conv3(x)\n",
        "#         x = self.bn3(x)\n",
        "#         x = F.relu(x)\n",
        "#         x_avg = self.avgpool3(x)\n",
        "#         x_max = self.maxpool3(x)\n",
        "#         x = torch.cat([x_avg, x_max], dim=1) # Concatenate output AvgPool dan MaxPool\n",
        "\n",
        "#         # Flatten the tensor for the fully connected layer\n",
        "#         x = x.view(-1, self.fc_input_features)\n",
        "#         x = self.dropout(x) # Terapkan Dropout sebelum layer FC\n",
        "#         x = self.fc(x)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1yuofyUHGrX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch # Pastikan torch diimpor untuk torch.cat\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Conv Block 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Conv Block 2 (in_channels sekarang 16 * 2 = 32)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16 * 2, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Conv Block 3 (in_channels sekarang 32 * 2 = 64)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32 * 2, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.avgpool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # --- TAMBAHAN: Global Average Pooling ---\n",
        "        # Ini akan mengambil output dari lapisan konvolusi/pooling terakhir\n",
        "        # dan mereduksinya menjadi 1x1 spasial per channel.\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # Output 1x1 per channel\n",
        "\n",
        "        # Hitung ukuran input untuk layer FC setelah Global Average Pooling\n",
        "        # Output channels dari conv3 adalah 64. Setelah pooling ganda dan concatenate, jadi 64 * 2 = 128 channels.\n",
        "        # Setelah Global Average Pooling (yang mengubah dimensi spasial menjadi 1x1),\n",
        "        # input ke FC hanya akan menjadi jumlah channels ini.\n",
        "        self.fc_input_features = (64 * 2) # Hanya jumlah channels, karena spasial sudah 1x1\n",
        "        self.fc = nn.Linear(self.fc_input_features, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Blok 1: Conv -> BatchNorm -> ReLU -> (AvgPool, MaxPool) -> Concat\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x_avg = self.avgpool1(x)\n",
        "        x_max = self.maxpool1(x)\n",
        "        x = torch.cat([x_avg, x_max], dim=1)\n",
        "\n",
        "        # Blok 2: Conv -> BatchNorm -> ReLU -> (AvgPool, MaxPool) -> Concat\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x_avg = self.avgpool2(x)\n",
        "        x_max = self.maxpool2(x)\n",
        "        x = torch.cat([x_avg, x_max], dim=1)\n",
        "\n",
        "        # Blok 3: Conv -> BatchNorm -> ReLU -> (AvgPool, MaxPool) -> Concat\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x_avg = self.avgpool3(x)\n",
        "        x_max = self.maxpool3(x)\n",
        "        x = torch.cat([x_avg, x_max], dim=1)\n",
        "\n",
        "        # --- APLIKASIKAN GLOBAL AVERAGE POOLING DI SINI ---\n",
        "        x = self.global_avg_pool(x) # Output: (batch_size, 128, 1, 1)\n",
        "\n",
        "        # Flatten the tensor for the fully connected layer\n",
        "        # Sekarang flatten lebih sederhana karena spasial sudah 1x1\n",
        "        x = x.view(x.size(0), -1) # Akan menjadi (batch_size, 128)\n",
        "\n",
        "        x = self.dropout(x) # Terapkan Dropout sebelum layer FC\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-05T04:04:59.253665Z",
          "iopub.status.busy": "2025-06-05T04:04:59.253483Z",
          "iopub.status.idle": "2025-06-05T04:04:59.267389Z",
          "shell.execute_reply": "2025-06-05T04:04:59.266787Z",
          "shell.execute_reply.started": "2025-06-05T04:04:59.253651Z"
        },
        "id": "15GmNBfHHGrX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # --- Di bagian bawah notebook Anda, setelah fungsi train_model, evaluate_model, dll. ---\n",
        "\n",
        "# print('---------------------------Data Asli (CNN Biasa)---------------------------')\n",
        "# # Inisialisasi model SimpleCNN baru untuk Data Asli\n",
        "# model_asli_cnn = SimpleCNN(num_classes=NUM_CLASSES)\n",
        "\n",
        "# early_stopping_asli_cnn = EarlyStopping(patience=10, verbose=True)\n",
        "# model_asli_cnn, history_asli_cnn = train_model(\n",
        "#     model=model_asli_cnn,\n",
        "#     train_loader=train_loader_asli,\n",
        "#     val_loader=val_loader_asli,\n",
        "#     early_stopping=early_stopping_asli_cnn,\n",
        "#     weight_decay=1e-4, # Anda mungkin ingin menyesuaikan weight_decay dan lr untuk CNN biasa\n",
        "#     best_model_path=\"best_model_asli_cnn.pth\"\n",
        "# )\n",
        "\n",
        "# plot_metrics(history_asli_cnn)\n",
        "# y_pred_asli_cnn, y_true_asli_cnn = evaluate_model(model_asli_cnn, test_loader_asli)\n",
        "# plot_confusion_matrix(y_true_asli_cnn, y_pred_asli_cnn, class_names)\n",
        "# plot_classification_metrics(y_true_asli_cnn, y_pred_asli_cnn, class_names)\n",
        "\n",
        "\n",
        "# print('---------------------------Data Asli + GAN (CNN Biasa)---------------------------')\n",
        "# # Inisialisasi model SimpleCNN baru untuk Data Asli + GAN\n",
        "# model_asli_gan_cnn = SimpleCNN(num_classes=NUM_CLASSES)\n",
        "\n",
        "# early_stopping_asli_gan_cnn = EarlyStopping(patience=10, verbose=True)\n",
        "# model_asli_gan_cnn, history_asli_gan_cnn = train_model(\n",
        "#     model=model_asli_gan_cnn,\n",
        "#     train_loader=train_loader_asli_gan,\n",
        "#     val_loader=val_loader_asli_gan,\n",
        "#     early_stopping=early_stopping_asli_gan_cnn,\n",
        "#     weight_decay=1e-4,\n",
        "#     best_model_path=\"best_model_asli_gan_cnn.pth\"\n",
        "# )\n",
        "\n",
        "# plot_metrics(history_asli_gan_cnn)\n",
        "# y_pred_asli_gan_cnn, y_true_asli_gan_cnn = evaluate_model(model_asli_gan_cnn, test_loader_asli_gan)\n",
        "# plot_confusion_matrix(y_true_asli_gan_cnn, y_pred_asli_gan_cnn, class_names)\n",
        "# plot_classification_metrics(y_true_asli_gan_cnn, y_pred_asli_gan_cnn, class_names)\n",
        "\n",
        "\n",
        "# print('---------------------------Data Asli + GAN + Ratio (CNN Biasa)---------------------------')\n",
        "# # Inisialisasi model SimpleCNN baru untuk Data Asli + GAN + Ratio\n",
        "# model_asli_gan_ratio_cnn = SimpleCNN(num_classes=NUM_CLASSES)\n",
        "\n",
        "# early_stopping_asli_gan_ratio_cnn = EarlyStopping(patience=10, verbose=True)\n",
        "# model_asli_gan_ratio_cnn, history_asli_gan_ratio_cnn = train_model(\n",
        "#     model=model_asli_gan_ratio_cnn,\n",
        "#     train_loader=train_loader_asli_gan_ratio,\n",
        "#     val_loader=val_loader_asli_gan_ratio,\n",
        "#     early_stopping=early_stopping_asli_gan_ratio_cnn,\n",
        "#     weight_decay=1e-4,\n",
        "#     best_model_path=\"best_model_asli_gan_ratio_cnn.pth\"\n",
        "# )\n",
        "\n",
        "# plot_metrics(history_asli_gan_ratio_cnn)\n",
        "# y_pred_asli_gan_ratio_cnn, y_true_asli_gan_ratio_cnn = evaluate_model(model_asli_gan_ratio_cnn, test_loader_asli_gan_ratio)\n",
        "# plot_confusion_matrix(y_true_asli_gan_ratio_cnn, y_pred_asli_gan_ratio_cnn, class_names)\n",
        "# plot_classification_metrics(y_true_asli_gan_ratio_cnn, y_pred_asli_gan_ratio_cnn, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-07T08:08:51.863664Z",
          "iopub.status.busy": "2025-06-07T08:08:51.863418Z",
          "iopub.status.idle": "2025-06-07T08:27:50.030176Z",
          "shell.execute_reply": "2025-06-07T08:27:50.028898Z",
          "shell.execute_reply.started": "2025-06-07T08:08:51.863642Z"
        },
        "id": "iBH95FdWHGrY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def run_cnn_experiment_automated_naming(patience_value, patience_name_suffix,\n",
        "                                       # DataLoaders sesuai penamaan Anda\n",
        "                                       train_loader_asli, val_loader_asli, test_loader_asli,\n",
        "                                       train_loader_asli_gan, val_loader_asli_gan, test_loader_asli_gan,\n",
        "                                       train_loader_asli_gan_ratio, val_loader_asli_gan_ratio, test_loader_asli_gan_ratio,\n",
        "                                       class_names, num_classes, device, weight_decay, num_images_to_show=37):\n",
        "    \"\"\"\n",
        "    Menjalankan eksperimen training dan evaluasi untuk CNN Biasa dengan penamaan variabel otomatis\n",
        "    berdasarkan konfigurasi early stopping, dan mengembalikan hasilnya.\n",
        "    \"\"\"\n",
        "\n",
        "    all_results_for_this_patience = {}\n",
        "\n",
        "    print(f'\\n---------------------------CNN Biasa (Early Stopping {patience_name_suffix})---------------------------')\n",
        "\n",
        "    # --- Case: Data Asli ---\n",
        "    print(f'---------------------------Data Asli (CNN Biasa - ES {patience_name_suffix})---------------------------')\n",
        "    # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "    early_stopping_instance_asli = None\n",
        "    if patience_value is not None:\n",
        "        early_stopping_instance_asli = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "    model_asli = SimpleCNN(num_classes=num_classes).to(device)\n",
        "    best_model_path_asli = f\"best_model_asli_cnn_{patience_name_suffix}.pth\"\n",
        "\n",
        "    model_asli, history_asli = train_model(\n",
        "        model=model_asli,\n",
        "        train_loader=train_loader_asli,\n",
        "        val_loader=val_loader_asli,\n",
        "        early_stopping=early_stopping_instance_asli, # Gunakan instance baru\n",
        "        weight_decay=weight_decay,\n",
        "        best_model_path=best_model_path_asli\n",
        "    )\n",
        "\n",
        "    plot_metrics(history_asli)\n",
        "    y_pred_asli, y_true_asli, mispredicted_data_asli = evaluate_model(model_asli, test_loader_asli)\n",
        "    plot_confusion_matrix(y_true_asli, y_pred_asli, class_names)\n",
        "    plot_classification_metrics(y_true_asli, y_pred_asli, class_names)\n",
        "    plot_mispredicted_images(mispredicted_data_asli, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "    all_results_for_this_patience['asli'] = {\n",
        "        'model': model_asli, 'history': history_asli, 'y_pred': y_pred_asli,\n",
        "        'y_true': y_true_asli, 'mispredicted_data': mispredicted_data_asli,\n",
        "        'best_model_path': best_model_path_asli\n",
        "    }\n",
        "\n",
        "    # --- Case: Data Asli + GAN ---\n",
        "    print(f'\\n---------------------------Data Asli + GAN (CNN Biasa - ES {patience_name_suffix})---------------------------')\n",
        "    # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "    early_stopping_instance_asli_gan = None\n",
        "    if patience_value is not None:\n",
        "        early_stopping_instance_asli_gan = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "    model_asli_gan = SimpleCNN(num_classes=num_classes).to(device)\n",
        "    best_model_path_asli_gan = f\"best_model_asli_gan_cnn_{patience_name_suffix}.pth\"\n",
        "\n",
        "    model_asli_gan, history_asli_gan = train_model(\n",
        "        model=model_asli_gan,\n",
        "        train_loader=train_loader_asli_gan,\n",
        "        val_loader=val_loader_asli_gan,\n",
        "        early_stopping=early_stopping_instance_asli_gan, # Gunakan instance baru\n",
        "        weight_decay=weight_decay,\n",
        "        best_model_path=best_model_path_asli_gan\n",
        "    )\n",
        "\n",
        "    plot_metrics(history_asli_gan)\n",
        "    y_pred_asli_gan, y_true_asli_gan, mispredicted_data_asli_gan = evaluate_model(model_asli_gan, test_loader_asli_gan)\n",
        "    plot_confusion_matrix(y_true_asli_gan, y_pred_asli_gan, class_names)\n",
        "    plot_classification_metrics(y_true_asli_gan, y_pred_asli_gan, class_names)\n",
        "    plot_mispredicted_images(mispredicted_data_asli_gan, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "    all_results_for_this_patience['asli_gan'] = {\n",
        "        'model': model_asli_gan, 'history': history_asli_gan, 'y_pred': y_pred_asli_gan,\n",
        "        'y_true': y_true_asli_gan, 'mispredicted_data': mispredicted_data_asli_gan,\n",
        "        'best_model_path': best_model_path_asli_gan\n",
        "    }\n",
        "\n",
        "    # --- Case: Data Asli + GAN + Ratio ---\n",
        "    print(f'\\n---------------------------Data Asli + GAN + Ratio (CNN Biasa - ES {patience_name_suffix})---------------------------')\n",
        "    # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "    early_stopping_instance_asli_gan_ratio = None\n",
        "    if patience_value is not None:\n",
        "        early_stopping_instance_asli_gan_ratio = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "    model_asli_gan_ratio = SimpleCNN(num_classes=num_classes).to(device)\n",
        "    best_model_path_asli_gan_ratio = f\"best_model_asli_gan_ratio_cnn_{patience_name_suffix}.pth\"\n",
        "\n",
        "    model_asli_gan_ratio, history_asli_gan_ratio = train_model(\n",
        "        model=model_asli_gan_ratio,\n",
        "        train_loader=train_loader_asli_gan_ratio,\n",
        "        val_loader=val_loader_asli_gan_ratio,\n",
        "        early_stopping=early_stopping_instance_asli_gan_ratio, # Gunakan instance baru\n",
        "        weight_decay=weight_decay,\n",
        "        best_model_path=best_model_path_asli_gan_ratio\n",
        "    )\n",
        "\n",
        "    plot_metrics(history_asli_gan_ratio)\n",
        "    y_pred_asli_gan_ratio, y_true_asli_gan_ratio, mispredicted_data_asli_gan_ratio = evaluate_model(model_asli_gan_ratio, test_loader_asli_gan_ratio)\n",
        "    plot_confusion_matrix(y_true_asli_gan_ratio, y_pred_asli_gan_ratio, class_names)\n",
        "    plot_classification_metrics(y_true_asli_gan_ratio, y_pred_asli_gan_ratio, class_names)\n",
        "    plot_mispredicted_images(mispredicted_data_asli_gan_ratio, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "    all_results_for_this_patience['asli_gan_ratio'] = {\n",
        "        'model': model_asli_gan_ratio, 'history': history_asli_gan_ratio, 'y_pred': y_pred_asli_gan_ratio,\n",
        "        'y_true': y_true_asli_gan_ratio, 'mispredicted_data': mispredicted_data_asli_gan_ratio,\n",
        "        'best_model_path': best_model_path_asli_gan_ratio\n",
        "    }\n",
        "\n",
        "    return all_results_for_this_patience\n",
        "\n",
        "# --- Panggilan Fungsi untuk Setiap Konfigurasi Early Stopping (tidak ada perubahan di sini) ---\n",
        "# ... (kode panggilan fungsi yang sudah ada) ...\n",
        "\n",
        "# --- Panggilan Fungsi untuk Setiap Konfigurasi Early Stopping ---\n",
        "\n",
        "# Membuat dictionary global untuk menyimpan semua hasil eksperimen\n",
        "# Contoh: all_experiment_results['es5']['asli']['model']\n",
        "all_experiment_results_cnn = {}\n",
        "\n",
        "# Panggil fungsi untuk setiap early stopping patience yang Anda inginkan\n",
        "early_stopping_patience_configs = {\n",
        "    \"es5\": EARLY_STOPPING_PATIENCE_5,\n",
        "    \"es10\": EARLY_STOPPING_PATIENCE_10,\n",
        "    \"es15\": EARLY_STOPPING_PATIENCE_15,\n",
        "    # \"es20\": EARLY_STOPPING_PATIENCE_20,\n",
        "    \"esNone\": EARLY_STOPPING_PATIENCE_NONE\n",
        "}\n",
        "\n",
        "for suffix, patience_val in early_stopping_patience_configs.items():\n",
        "    all_experiment_results_cnn[suffix] = run_cnn_experiment_automated_naming(patience_val, suffix,\n",
        "                                                                             train_loader_asli, val_loader_asli, test_loader_asli,\n",
        "                                                                             train_loader_asli_gan, val_loader_asli_gan, test_loader_asli_gan,\n",
        "                                                                             train_loader_asli_gan_ratio, val_loader_asli_gan_ratio, test_loader_asli_gan_ratio,\n",
        "                                                                             class_names, NUM_CLASSES, DEVICE, WEIGHT_DECAY)\n",
        "\n",
        "# Sekarang Anda bisa mengakses hasilnya seperti ini:\n",
        "# model_es5_asli = all_experiment_results_cnn['es5']['asli']['model']\n",
        "# history_es10_asli_gan = all_experiment_results_cnn['es10']['asli_gan']['history']\n",
        "# y_pred_esNone_asli_gan_ratio = all_experiment_results_cnn['esNone']['asli_gan_ratio']['y_pred']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHmF1BMvHGrY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# import timm # Pastikan timm diimpor\n",
        "# import torch.nn as nn # Pastikan nn diimpor\n",
        "# import random # Pastikan random diimpor\n",
        "# import numpy as np # Pastikan numpy diimpor\n",
        "# import matplotlib.pyplot as plt # Pastikan matplotlib diimpor\n",
        "# import seaborn as sns # Pastikan seaborn diimpor\n",
        "\n",
        "# # Pastikan class_names, NUM_CLASSES, dan device sudah didefinisikan.\n",
        "# # Pastikan fungsi train_model, evaluate_model, plot_metrics, plot_confusion_matrix,\n",
        "# # plot_classification_metrics, dan plot_mispredicted_images sudah didefinisikan dengan benar.\n",
        "# # Pastikan EarlyStopping sudah didefinisikan (dengan min_delta opsional).\n",
        "\n",
        "# # --- Pastikan DataLoader Anda sudah ada dan terkonfigurasi dengan benar ---\n",
        "# # train_loader_asli\n",
        "# # val_loader_asli\n",
        "# # test_loader_asli\n",
        "# # train_loader_asli_gan\n",
        "# # val_loader_asli_gan\n",
        "# # test_loader_asli_gan\n",
        "# # train_loader_asli_gan_ratio\n",
        "# # val_loader_asli_gan_ratio\n",
        "# # test_loader_asli_gan_ratio\n",
        "\n",
        "\n",
        "# def run_resnet_experiment_automated_naming(patience_value, patience_name_suffix,\n",
        "#                                           # DataLoaders sesuai penamaan Anda\n",
        "#                                           train_loader_asli, val_loader_asli, test_loader_asli,\n",
        "#                                           train_loader_asli_gan, val_loader_asli_gan, test_loader_asli_gan,\n",
        "#                                           train_loader_asli_gan_ratio, val_loader_asli_gan_ratio, test_loader_asli_gan_ratio,\n",
        "#                                           class_names, num_classes, device, weight_decay, num_images_to_show=37):\n",
        "#     \"\"\"\n",
        "#     Menjalankan eksperimen training dan evaluasi untuk ResNet50 dengan penamaan variabel otomatis\n",
        "#     berdasarkan konfigurasi early stopping.\n",
        "\n",
        "#     Args:\n",
        "#         patience_value: Nilai patience untuk EarlyStopping, atau None untuk menonaktifkan.\n",
        "#         patience_name_suffix: String suffix untuk penamaan variabel (misal: \"es5\", \"esNone\").\n",
        "#         train_loader_asli, val_loader_asli, test_loader_asli: DataLoaders untuk dataset asli.\n",
        "#         train_loader_asli_gan, val_loader_asli_gan, test_loader_asli_gan: DataLoaders untuk dataset asli + GAN.\n",
        "#         train_loader_asli_gan_ratio, val_loader_asli_gan_ratio, test_loader_asli_gan_ratio: DataLoaders untuk dataset asli + GAN + Ratio.\n",
        "#         class_names: List nama kelas.\n",
        "#         num_classes: Jumlah kelas.\n",
        "#         device: Device (cuda/cpu) untuk training.\n",
        "#         weight_decay: Weight decay untuk optimizer.\n",
        "#         num_images_to_show: Jumlah gambar mispredicted yang akan ditampilkan.\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(f'\\n---------------------------ResNet50 (TIMM) (Early Stopping {patience_name_suffix})---------------------------')\n",
        "\n",
        "#     # --- Case: Data Asli ---\n",
        "#     print(f'---------------------------Data Asli (ResNet50 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_resnet = timm.create_model('resnet50', pretrained=False) # Menggunakan pretrained=False sesuai kode Anda\n",
        "#     # model_asli_resnet.fc = nn.Linear(model_asli_resnet.fc.in_features, num_classes).to(device) # Mengganti layer .fc untuk ResNet dan pindahkan ke device\n",
        "#     model_asli_resnet.fc = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_resnet.fc.in_features, num_classes)\n",
        "#     ).to(device) # Pindahkan seluruh Sequential ke device\n",
        "\n",
        "#     best_model_path_asli = f\"best_model_asli_resnet_{patience_name_suffix}.pth\"\n",
        "\n",
        "#     model_asli_resnet, history_asli_resnet = train_model(\n",
        "#         model=model_asli_resnet,\n",
        "#         train_loader=train_loader_asli,\n",
        "#         val_loader=val_loader_asli,\n",
        "#         early_stopping=early_stopping_instance_asli, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=best_model_path_asli\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_resnet)\n",
        "#     y_pred_asli_resnet, y_true_asli_resnet, mispredicted_data_asli_resnet = evaluate_model(model_asli_resnet, test_loader_asli)\n",
        "#     plot_confusion_matrix(y_true_asli_resnet, y_pred_asli_resnet, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_resnet, y_pred_asli_resnet, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_resnet, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "#     # --- Case: Data Asli + GAN ---\n",
        "#     print(f'\\n---------------------------Data Asli + GAN (ResNet50 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli_gan = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli_gan = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_gan_resnet = timm.create_model('resnet50', pretrained=False) # Menggunakan pretrained=False sesuai kode Anda\n",
        "#     # model_asli_gan_resnet.fc = nn.Linear(model_asli_gan_resnet.fc.in_features, num_classes).to(device) # Mengganti layer .fc untuk ResNet dan pindahkan ke device\n",
        "#     model_asli_gan_resnet.fc = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_gan_resnet.fc.in_features, num_classes)\n",
        "#     ).to(device)\n",
        "\n",
        "#     model_asli_gan_resnet, history_asli_gan_resnet = train_model(\n",
        "#         model=model_asli_gan_resnet,\n",
        "#         train_loader=train_loader_asli_gan,\n",
        "#         val_loader=val_loader_asli_gan, # Menggunakan val_loader_asli_gan (sesuai instruksi Anda)\n",
        "#         early_stopping=early_stopping_instance_asli_gan, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=f\"best_model_asli_gan_resnet_{patience_name_suffix}.pth\"\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_gan_resnet)\n",
        "#     y_pred_asli_gan_resnet, y_true_asli_gan_resnet, mispredicted_data_asli_gan_resnet = evaluate_model(model_asli_gan_resnet, test_loader_asli_gan) # Menggunakan test_loader_asli_gan (sesuai instruksi Anda)\n",
        "#     plot_confusion_matrix(y_true_asli_gan_resnet, y_pred_asli_gan_resnet, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_gan_resnet, y_pred_asli_gan_resnet, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_gan_resnet, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "#     # --- Case: Data Asli + GAN + Ratio ---\n",
        "#     print(f'\\n---------------------------Data Asli + GAN + Ratio (ResNet50 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli_gan_ratio = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli_gan_ratio = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_gan_ratio_resnet = timm.create_model('resnet50', pretrained=False) # Menggunakan pretrained=False sesuai kode Anda\n",
        "#     # model_asli_gan_ratio_resnet.fc = nn.Linear(model_asli_gan_ratio_resnet.fc.in_features, num_classes).to(device) # Mengganti layer .fc untuk ResNet dan pindahkan ke device\n",
        "#     model_asli_gan_ratio_resnet.fc = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_gan_ratio_resnet.fc.in_features, num_classes)\n",
        "#     ).to(device)\n",
        "#     model_asli_gan_ratio_resnet, history_asli_gan_ratio_resnet = train_model(\n",
        "#         model=model_asli_gan_ratio_resnet,\n",
        "#         train_loader=train_loader_asli_gan_ratio,\n",
        "#         val_loader=val_loader_asli_gan_ratio,     # Menggunakan val_loader_asli_gan_ratio (sesuai instruksi Anda)\n",
        "#         early_stopping=early_stopping_instance_asli_gan_ratio, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=f\"best_model_asli_gan_ratio_resnet_{patience_name_suffix}.pth\"\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_gan_ratio_resnet)\n",
        "#     y_pred_asli_gan_ratio_resnet, y_true_asli_gan_ratio_resnet, mispredicted_data_asli_gan_ratio_resnet = evaluate_model(model_asli_gan_ratio_resnet, test_loader_asli_gan_ratio) # Menggunakan test_loader_asli_gan_ratio (sesuai instruksi Anda)\n",
        "#     plot_confusion_matrix(y_true_asli_gan_ratio_resnet, y_pred_asli_gan_ratio_resnet, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_gan_ratio_resnet, y_pred_asli_gan_ratio_resnet, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_gan_ratio_resnet, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "# all_experiment_results_resnet = {}\n",
        "\n",
        "# early_stopping_patience_configs = {\n",
        "#     \"es5\": EARLY_STOPPING_PATIENCE_5,\n",
        "#     \"es10\": EARLY_STOPPING_PATIENCE_10,\n",
        "#     \"es15\": EARLY_STOPPING_PATIENCE_15,\n",
        "#     # \"es20\": EARLY_STOPPING_PATIENCE_20,\n",
        "#     \"esNone\": EARLY_STOPPING_PATIENCE_NONE\n",
        "# }\n",
        "\n",
        "# for suffix, patience_val in early_stopping_patience_configs.items():\n",
        "#     all_experiment_results_resnet[suffix] = run_resnet_experiment_automated_naming(patience_val, suffix,\n",
        "#                                                                                  train_loader_asli, val_loader_asli, test_loader_asli,\n",
        "#                                                                                  train_loader_asli_gan, val_loader_asli_gan, test_loader_asli_gan,\n",
        "#                                                                                  train_loader_asli_gan_ratio, val_loader_asli_gan_ratio, test_loader_asli_gan_ratio,\n",
        "#                                                                                  class_names, NUM_CLASSES, DEVICE, WEIGHT_DECAY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLZcwmCEHGrY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def run_pretrained_resnet_experiment_automated_naming(patience_value, patience_name_suffix,\n",
        "#                                                    # DataLoaders sesuai penamaan Anda\n",
        "#                                                    train_loader_asli_pretrained_resnet, val_loader_asli_pretrained_resnet, test_loader_asli_pretrained_resnet,\n",
        "#                                                    train_loader_asli_gan_pretrained_resnet, val_loader_asli_gan_pretrained_resnet, test_loader_asli_gan_pretrained_resnet,\n",
        "#                                                    train_loader_asli_gan_ratio_pretrained_resnet, val_loader_asli_gan_ratio_pretrained_resnet, test_loader_asli_gan_ratio_pretrained_resnet,\n",
        "#                                                    class_names, num_classes, device, weight_decay, num_images_to_show=37):\n",
        "#     \"\"\"\n",
        "#     Menjalankan eksperimen training dan evaluasi untuk ResNet50 (Pretrained) dengan penamaan variabel otomatis\n",
        "#     berdasarkan konfigurasi early stopping.\n",
        "\n",
        "#     Args:\n",
        "#         patience_value: Nilai patience untuk EarlyStopping, atau None untuk menonaktifkan.\n",
        "#         patience_name_suffix: String suffix untuk penamaan variabel (misal: \"es5\", \"esNone\").\n",
        "#         ... (parameter DataLoaders sesuai nama yang Anda berikan) ...\n",
        "#         class_names: List nama kelas.\n",
        "#         num_classes: Jumlah kelas.\n",
        "#         device: Device (cuda/cpu) untuk training.\n",
        "#         weight_decay: Weight decay untuk optimizer.\n",
        "#         num_images_to_show: Jumlah gambar mispredicted yang akan ditampilkan.\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(f'\\n---------------------------Pretrained ResNet50 (TIMM) (Early Stopping {patience_name_suffix})---------------------------')\n",
        "\n",
        "#     # --- Case: Data Asli ---\n",
        "#     print(f'---------------------------Data Asli (Pretrained ResNet50 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_pretrained_resnet = timm.create_model('resnet50', pretrained=True) # Menggunakan pretrained=True\n",
        "#     # model_asli_pretrained_resnet.fc = nn.Linear(model_asli_pretrained_resnet.fc.in_features, num_classes).to(device) # Mengganti layer .fc untuk ResNet dan pindahkan ke device\n",
        "#     model_asli_pretrained_resnet.fc = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_pretrained_resnet.fc.in_features, num_classes)\n",
        "#     ).to(device) # Pindahkan seluruh Sequential ke device\n",
        "\n",
        "#     best_model_path_asli = f\"best_model_asli_pretrained_resnet_{patience_name_suffix}.pth\"\n",
        "\n",
        "#     model_asli_pretrained_resnet, history_asli_pretrained_resnet = train_model(\n",
        "#         model=model_asli_pretrained_resnet,\n",
        "#         train_loader=train_loader_asli_pretrained_resnet,\n",
        "#         val_loader=val_loader_asli_pretrained_resnet,\n",
        "#         early_stopping=early_stopping_instance_asli, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=best_model_path_asli\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_pretrained_resnet)\n",
        "#     y_pred_asli_pretrained_resnet, y_true_asli_pretrained_resnet, mispredicted_data_asli_pretrained_resnet = evaluate_model(model_asli_pretrained_resnet, test_loader_asli_pretrained_resnet)\n",
        "#     plot_confusion_matrix(y_true_asli_pretrained_resnet, y_pred_asli_pretrained_resnet, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_pretrained_resnet, y_pred_asli_pretrained_resnet, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_pretrained_resnet, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "#     # --- Case: Data Asli + GAN ---\n",
        "#     print(f'\\n---------------------------Data Asli + GAN (Pretrained ResNet50 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli_gan = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli_gan = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_gan_pretrained_resnet = timm.create_model('resnet50', pretrained=True)\n",
        "#     # model_asli_gan_pretrained_resnet.fc = nn.Linear(model_asli_gan_pretrained_resnet.fc.in_features, num_classes).to(device)\n",
        "#     model_asli_gan_pretrained_resnet.fc = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_gan_pretrained_resnet.fc.in_features, num_classes)\n",
        "#     ).to(device)\n",
        "#     model_asli_gan_pretrained_resnet, history_asli_gan_pretrained_resnet = train_model(\n",
        "#         model=model_asli_gan_pretrained_resnet,\n",
        "#         train_loader=train_loader_asli_gan_pretrained_resnet,\n",
        "#         val_loader=val_loader_asli_gan_pretrained_resnet, # Menggunakan val_loader_asli_gan_pretrained_resnet\n",
        "#         early_stopping=early_stopping_instance_asli_gan, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=f\"best_model_asli_gan_pretrained_resnet_{patience_name_suffix}.pth\"\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_gan_pretrained_resnet)\n",
        "#     y_pred_asli_gan_pretrained_resnet, y_true_asli_gan_pretrained_resnet, mispredicted_data_asli_gan_pretrained_resnet = evaluate_model(model_asli_gan_pretrained_resnet, test_loader_asli_gan_pretrained_resnet) # Menggunakan test_loader_asli_gan_pretrained_resnet\n",
        "#     plot_confusion_matrix(y_true_asli_gan_pretrained_resnet, y_pred_asli_gan_pretrained_resnet, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_gan_pretrained_resnet, y_pred_asli_gan_pretrained_resnet, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_gan_pretrained_resnet, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "#     # --- Case: Data Asli + GAN + Ratio ---\n",
        "#     print(f'\\n---------------------------Data Asli + GAN + Ratio (Pretrained ResNet50 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli_gan_ratio = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli_gan_ratio = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_gan_ratio_pretrained_resnet = timm.create_model('resnet50', pretrained=True)\n",
        "#     # model_asli_gan_ratio_pretrained_resnet.fc = nn.Linear(model_asli_gan_ratio_pretrained_resnet.fc.in_features, num_classes).to(device)\n",
        "#     model_asli_gan_ratio_pretrained_resnet.fc = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_gan_ratio_pretrained_resnet.fc.in_features, num_classes)\n",
        "#     ).to(device)\n",
        "\n",
        "#     model_asli_gan_ratio_pretrained_resnet, history_asli_gan_ratio_pretrained_resnet = train_model(\n",
        "#         model=model_asli_gan_ratio_pretrained_resnet,\n",
        "#         train_loader=train_loader_asli_gan_ratio_pretrained_resnet,\n",
        "#         val_loader=val_loader_asli_gan_ratio_pretrained_resnet, # Menggunakan val_loader_asli_gan_ratio_pretrained_resnet\n",
        "#         early_stopping=early_stopping_instance_asli_gan_ratio, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=f\"best_model_asli_gan_ratio_pretrained_resnet_{patience_name_suffix}.pth\"\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_gan_ratio_pretrained_resnet)\n",
        "#     y_pred_asli_gan_ratio_pretrained_resnet, y_true_asli_gan_ratio_pretrained_resnet, mispredicted_data_asli_gan_ratio_pretrained_resnet = evaluate_model(model_asli_gan_ratio_pretrained_resnet, test_loader_asli_gan_ratio_pretrained_resnet)\n",
        "#     plot_confusion_matrix(y_true_asli_gan_ratio_pretrained_resnet, y_pred_asli_gan_ratio_pretrained_resnet, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_gan_ratio_pretrained_resnet, y_pred_asli_gan_ratio_pretrained_resnet, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_gan_ratio_pretrained_resnet, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "# all_experiment_results_pretrained_resnet = {}\n",
        "\n",
        "# early_stopping_patience_configs = {\n",
        "#     \"es5\": EARLY_STOPPING_PATIENCE_5,\n",
        "#     \"es10\": EARLY_STOPPING_PATIENCE_10,\n",
        "#     \"es15\": EARLY_STOPPING_PATIENCE_15,\n",
        "#     # \"es20\": EARLY_STOPPING_PATIENCE_20,\n",
        "#     \"esNone\": EARLY_STOPPING_PATIENCE_NONE\n",
        "# }\n",
        "\n",
        "# for suffix, patience_val in early_stopping_patience_configs.items():\n",
        "#     all_experiment_results_pretrained_resnet[suffix] = run_pretrained_resnet_experiment_automated_naming(patience_val, suffix,\n",
        "#                                                                                  train_loader_asli_pretrained_resnet, val_loader_asli_pretrained_resnet, test_loader_asli_pretrained_resnet,\n",
        "#                                                                                  train_loader_asli_gan_pretrained_resnet, val_loader_asli_gan_pretrained_resnet, test_loader_asli_gan_pretrained_resnet,\n",
        "#                                                                                  train_loader_asli_gan_ratio_pretrained_resnet, val_loader_asli_gan_ratio_pretrained_resnet, test_loader_asli_gan_ratio_pretrained_resnet,\n",
        "#                                                                                  class_names, NUM_CLASSES, DEVICE, WEIGHT_DECAY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h-386g2HGrd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def run_effnet_experiment_automated_naming(patience_value, patience_name_suffix,\n",
        "#                                           # DataLoaders sesuai penamaan Anda\n",
        "#                                           train_loader_asli, val_loader_asli, test_loader_asli,\n",
        "#                                           train_loader_asli_gan, val_loader_asli_gan, test_loader_asli_gan,\n",
        "#                                           train_loader_asli_gan_ratio, val_loader_asli_gan_ratio, test_loader_asli_gan_ratio,\n",
        "#                                           class_names, num_classes, device, weight_decay, num_images_to_show=37):\n",
        "#     \"\"\"\n",
        "#     Menjalankan eksperimen training dan evaluasi untuk EfficientNetV2-B0 dengan penamaan variabel otomatis\n",
        "#     berdasarkan konfigurasi early stopping.\n",
        "\n",
        "#     Args:\n",
        "#         patience_value: Nilai patience untuk EarlyStopping, atau None untuk menonaktifkan.\n",
        "#         patience_name_suffix: String suffix untuk penamaan variabel (misal: \"es5\", \"esNone\").\n",
        "#         ... (parameter DataLoaders sesuai nama yang Anda berikan) ...\n",
        "#         class_names: List nama kelas.\n",
        "#         num_classes: Jumlah kelas.\n",
        "#         device: Device (cuda/cpu) untuk training.\n",
        "#         weight_decay: Weight decay untuk optimizer.\n",
        "#         num_images_to_show: Jumlah gambar mispredicted yang akan ditampilkan.\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(f'\\n---------------------------EfficientNetV2-B0 (TIMM) (Early Stopping {patience_name_suffix})---------------------------')\n",
        "\n",
        "#     # --- Case: Data Asli ---\n",
        "#     print(f'---------------------------Data Asli (EfficientNetV2-B0 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_effnet = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=False) # Menggunakan pretrained=False sesuai kode Anda\n",
        "#     # model_asli_effnet.classifier = nn.Linear(model_asli_effnet.classifier.in_features, num_classes).to(device) # Mengganti layer .classifier dan pindahkan ke device\n",
        "#     model_asli_effnet.classifier = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_effnet.classifier.in_features, num_classes)\n",
        "#     ).to(device) # Pindahkan seluruh Sequential ke device\n",
        "#     best_model_path_asli = f\"best_model_asli_effnet_{patience_name_suffix}.pth\"\n",
        "\n",
        "#     model_asli_effnet, history_asli_effnet = train_model(\n",
        "#         model=model_asli_effnet,\n",
        "#         train_loader=train_loader_asli,\n",
        "#         val_loader=val_loader_asli,\n",
        "#         early_stopping=early_stopping_instance_asli, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=best_model_path_asli\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_effnet)\n",
        "#     y_pred_asli_effnet, y_true_asli_effnet, mispredicted_data_asli_effnet = evaluate_model(model_asli_effnet, test_loader_asli)\n",
        "#     plot_confusion_matrix(y_true_asli_effnet, y_pred_asli_effnet, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_effnet, y_pred_asli_effnet, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_effnet, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "#     # --- Case: Data Asli + GAN ---\n",
        "#     print(f'\\n---------------------------Data Asli + GAN (EfficientNetV2-B0 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli_gan = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli_gan = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_gan_effnet = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=False) # Menggunakan pretrained=False sesuai kode Anda\n",
        "#     # model_asli_gan_effnet.classifier = nn.Linear(model_asli_gan_effnet.classifier.in_features, num_classes).to(device)\n",
        "#     model_asli_gan_effnet.classifier = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_gan_effnet.classifier.in_features, num_classes)\n",
        "#     ).to(device)\n",
        "\n",
        "#     model_asli_gan_effnet, history_asli_gan_effnet = train_model(\n",
        "#         model=model_asli_gan_effnet,\n",
        "#         train_loader=train_loader_asli_gan,\n",
        "#         val_loader=val_loader_asli_gan, # Menggunakan val_loader_asli_gan (sesuai instruksi Anda)\n",
        "#         early_stopping=early_stopping_instance_asli_gan, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=f\"best_model_asli_gan_effnet_{patience_name_suffix}.pth\"\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_gan_effnet)\n",
        "#     y_pred_asli_gan_effnet, y_true_asli_gan_effnet, mispredicted_data_asli_gan_effnet = evaluate_model(model_asli_gan_effnet, test_loader_asli_gan) # Menggunakan test_loader_asli_gan (sesuai instruksi Anda)\n",
        "#     plot_confusion_matrix(y_true_asli_gan_effnet, y_pred_asli_gan_effnet, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_gan_effnet, y_pred_asli_gan_effnet, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_gan_effnet, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "#     # --- Case: Data Asli + GAN + Ratio ---\n",
        "#     print(f'\\n---------------------------Data Asli + GAN + Ratio (EfficientNetV2-B0 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli_gan_ratio = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli_gan_ratio = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_gan_ratio_effnet = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=False) # Menggunakan pretrained=False sesuai kode Anda\n",
        "#     # model_asli_gan_ratio_effnet.classifier = nn.Linear(model_asli_gan_ratio_effnet.classifier.in_features, num_classes).to(device)\n",
        "#     model_asli_gan_ratio_effnet.classifier = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_gan_ratio_effnet.classifier.in_features, num_classes)\n",
        "#     ).to(device)\n",
        "\n",
        "#     model_asli_gan_ratio_effnet, history_asli_gan_ratio_effnet = train_model(\n",
        "#         model=model_asli_gan_ratio_effnet,\n",
        "#         train_loader=train_loader_asli_gan_ratio,\n",
        "#         val_loader=val_loader_asli_gan_ratio,     # Menggunakan val_loader_asli_gan_ratio (sesuai instruksi Anda)\n",
        "#         early_stopping=early_stopping_instance_asli_gan_ratio, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=f\"best_model_asli_gan_ratio_effnet_{patience_name_suffix}.pth\"\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_gan_ratio_effnet)\n",
        "#     y_pred_asli_gan_ratio_effnet, y_true_asli_gan_ratio_effnet, mispredicted_data_asli_gan_ratio_effnet = evaluate_model(model_asli_gan_ratio_effnet, test_loader_asli_gan_ratio) # Menggunakan test_loader_asli_gan_ratio (sesuai instruksi Anda)\n",
        "#     plot_confusion_matrix(y_true_asli_gan_ratio_effnet, y_pred_asli_gan_ratio_effnet, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_gan_ratio_effnet, y_pred_asli_gan_ratio_effnet, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_gan_ratio_effnet, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "# all_experiment_results_effnet={}\n",
        "\n",
        "# early_stopping_patience_configs = {\n",
        "#     \"es5\": EARLY_STOPPING_PATIENCE_5,\n",
        "#     \"es10\": EARLY_STOPPING_PATIENCE_10,\n",
        "#     \"es15\": EARLY_STOPPING_PATIENCE_15,\n",
        "#     # \"es20\": EARLY_STOPPING_PATIENCE_20,\n",
        "#     \"esNone\": EARLY_STOPPING_PATIENCE_NONE\n",
        "# }\n",
        "\n",
        "# for suffix, patience_val in early_stopping_patience_configs.items():\n",
        "#     all_experiment_results_effnet[suffix] = run_effnet_experiment_automated_naming(patience_val, suffix,\n",
        "#                                                                                 train_loader_asli, val_loader_asli, test_loader_asli,\n",
        "#                                                                                 train_loader_asli_gan, val_loader_asli_gan, test_loader_asli_gan,\n",
        "#                                                                                 train_loader_asli_gan_ratio, val_loader_asli_gan_ratio, test_loader_asli_gan_ratio,\n",
        "#                                                                                 class_names, NUM_CLASSES, DEVICE, WEIGHT_DECAY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-05T12:30:26.030422Z",
          "iopub.status.busy": "2025-06-05T12:30:26.029651Z",
          "iopub.status.idle": "2025-06-05T12:44:36.803959Z",
          "shell.execute_reply": "2025-06-05T12:44:36.802982Z",
          "shell.execute_reply.started": "2025-06-05T12:30:26.030396Z"
        },
        "id": "haIHPcmVHGrd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# def run_pretrained_effnet_experiment_automated_naming(patience_value, patience_name_suffix,\n",
        "#                                                       # DataLoaders sesuai penamaan Anda (dengan _pretrained_efficientnet)\n",
        "#                                                       train_loader_asli_pretrained_efficientnet, val_loader_asli_pretrained_efficientnet, test_loader_asli_pretrained_efficientnet,\n",
        "#                                                       train_loader_asli_gan_pretrained_efficientnet, val_loader_asli_gan_pretrained_efficientnet, test_loader_asli_gan_pretrained_efficientnet,\n",
        "#                                                       train_loader_asli_gan_ratio_pretrained_efficientnet, val_loader_asli_gan_ratio_pretrained_efficientnet, test_loader_asli_gan_ratio_pretrained_efficientnet,\n",
        "#                                                       class_names, num_classes, device, weight_decay, num_images_to_show=37):\n",
        "#     \"\"\"\n",
        "#     Menjalankan eksperimen training dan evaluasi untuk EfficientNetV2-B0 (Pretrained) dengan penamaan variabel otomatis\n",
        "#     berdasarkan konfigurasi early stopping.\n",
        "\n",
        "#     Args:\n",
        "#         patience_value: Nilai patience untuk EarlyStopping, atau None untuk menonaktifkan.\n",
        "#         patience_name_suffix: String suffix untuk penamaan variabel (misal: \"es5\", \"esNone\").\n",
        "#         ... (parameter DataLoaders sesuai nama yang Anda berikan) ...\n",
        "#         class_names: List nama kelas.\n",
        "#         num_classes: Jumlah kelas.\n",
        "#         device: Device (cuda/cpu) untuk training.\n",
        "#         weight_decay: Weight decay untuk optimizer.\n",
        "#         num_images_to_show: Jumlah gambar mispredicted yang akan ditampilkan.\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(f'\\n---------------------------Pretrained EfficientNetV2-B0 (TIMM) (Early Stopping {patience_name_suffix})---------------------------')\n",
        "\n",
        "#     # --- Case: Data Asli ---\n",
        "#     print(f'---------------------------Data Asli (Pretrained EfficientNetV2-B0 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_pretrained_effnet = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=True) # Menggunakan pretrained=True\n",
        "#     # model_asli_pretrained_effnet.classifier = nn.Linear(model_asli_pretrained_effnet.classifier.in_features, num_classes).to(device) # Mengganti layer .classifier dan pindahkan ke device\n",
        "#     model_asli_pretrained_effnet.classifier = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_pretrained_effnet.classifier.in_features, num_classes)\n",
        "#     ).to(device) # Pindahkan seluruh Sequential ke device\n",
        "\n",
        "#     best_model_path_asli = f\"best_model_asli_pretrained_effnet_{patience_name_suffix}.pth\"\n",
        "\n",
        "#     model_asli_pretrained_effnet, history_asli_effnet_pretrained = train_model(\n",
        "#         model=model_asli_pretrained_effnet,\n",
        "#         train_loader=train_loader_asli_pretrained_efficientnet,\n",
        "#         val_loader=val_loader_asli_pretrained_efficientnet,\n",
        "#         early_stopping=early_stopping_instance_asli, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=best_model_path_asli\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_effnet_pretrained)\n",
        "#     y_pred_asli_effnet_pretrained, y_true_asli_effnet_pretrained, mispredicted_data_asli_effnet_pretrained = evaluate_model(model_asli_pretrained_effnet, test_loader_asli_pretrained_efficientnet)\n",
        "#     plot_confusion_matrix(y_true_asli_effnet_pretrained, y_pred_asli_effnet_pretrained, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_effnet_pretrained, y_pred_asli_effnet_pretrained, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_effnet_pretrained, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "#     # --- Case: Data Asli + GAN ---\n",
        "#     print(f'\\n---------------------------Data Asli + GAN (Pretrained EfficientNetV2-B0 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli_gan = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli_gan = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_gan_pretrained_effnet = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=True)\n",
        "#     # model_asli_gan_pretrained_effnet.classifier = nn.Linear(model_asli_gan_pretrained_effnet.classifier.in_features, num_classes).to(device)\n",
        "#     model_asli_gan_pretrained_effnet.classifier = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_gan_pretrained_effnet.classifier.in_features, num_classes)\n",
        "#     ).to(device)\n",
        "\n",
        "#     model_asli_gan_pretrained_effnet, history_asli_gan_effnet_pretrained = train_model(\n",
        "#         model=model_asli_gan_pretrained_effnet,\n",
        "#         train_loader=train_loader_asli_gan_pretrained_efficientnet,\n",
        "#         val_loader=val_loader_asli_gan_pretrained_efficientnet, # Menggunakan val_loader_asli_gan_pretrained_efficientnet\n",
        "#         early_stopping=early_stopping_instance_asli_gan, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=f\"best_model_asli_gan_pretrained_effnet_{patience_name_suffix}.pth\"\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_gan_effnet_pretrained)\n",
        "#     y_pred_asli_gan_effnet_pretrained, y_true_asli_gan_effnet_pretrained, mispredicted_data_asli_gan_effnet_pretrained = evaluate_model(model_asli_gan_pretrained_effnet, test_loader_asli_gan_pretrained_efficientnet) # Menggunakan test_loader_asli_gan_pretrained_efficientnet\n",
        "#     plot_confusion_matrix(y_true_asli_gan_effnet_pretrained, y_pred_asli_gan_effnet_pretrained, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_gan_effnet_pretrained, y_pred_asli_gan_effnet_pretrained, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_gan_effnet_pretrained, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "#     # --- Case: Data Asli + GAN + Ratio ---\n",
        "#     print(f'\\n---------------------------Data Asli + GAN + Ratio (Pretrained EfficientNetV2-B0 - ES {patience_name_suffix})---------------------------')\n",
        "#     # Inisialisasi early_stopping_instance DI SINI untuk setiap case dataset\n",
        "#     early_stopping_instance_asli_gan_ratio = None\n",
        "#     if patience_value is not None:\n",
        "#         early_stopping_instance_asli_gan_ratio = EarlyStopping(patience=patience_value, verbose=True)\n",
        "\n",
        "#     model_asli_gan_ratio_pretrained_effnet = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', pretrained=True)\n",
        "#     # model_asli_gan_ratio_pretrained_effnet.classifier = nn.Linear(model_asli_gan_ratio_pretrained_effnet.classifier.in_features, num_classes).to(device)\n",
        "#     model_asli_gan_ratio_pretrained_effnet.classifier = nn.Sequential(\n",
        "#         nn.Dropout(0.5), # Probabilitas Dropout, bisa disesuaikan\n",
        "#         nn.Linear(model_asli_gan_ratio_pretrained_effnet.classifier.in_features, num_classes)\n",
        "#     ).to(device)\n",
        "\n",
        "#     model_asli_gan_ratio_pretrained_effnet, history_asli_gan_ratio_effnet_pretrained = train_model(\n",
        "#         model=model_asli_gan_ratio_pretrained_effnet,\n",
        "#         train_loader=train_loader_asli_gan_ratio_pretrained_efficientnet,\n",
        "#         val_loader=val_loader_asli_gan_ratio_pretrained_efficientnet, # Menggunakan val_loader_asli_gan_ratio_pretrained_efficientnet\n",
        "#         early_stopping=early_stopping_instance_asli_gan_ratio, # Gunakan instance baru\n",
        "#         weight_decay=weight_decay,\n",
        "#         best_model_path=f\"best_model_asli_gan_ratio_pretrained_effnet_{patience_name_suffix}.pth\"\n",
        "#     )\n",
        "\n",
        "#     plot_metrics(history_asli_gan_ratio_effnet_pretrained)\n",
        "#     y_pred_asli_gan_ratio_effnet_pretrained, y_true_asli_gan_ratio_effnet_pretrained, mispredicted_data_asli_gan_ratio_effnet_pretrained = evaluate_model(model_asli_gan_ratio_pretrained_effnet, test_loader_asli_gan_ratio_pretrained_efficientnet)\n",
        "#     plot_confusion_matrix(y_true_asli_gan_ratio_effnet_pretrained, y_pred_asli_gan_ratio_effnet_pretrained, class_names)\n",
        "#     plot_classification_metrics(y_true_asli_gan_ratio_effnet_pretrained, y_pred_asli_gan_ratio_effnet_pretrained, class_names)\n",
        "#     plot_mispredicted_images(mispredicted_data_asli_gan_ratio_effnet_pretrained, class_names, num_images_to_show=num_images_to_show)\n",
        "\n",
        "# all_experiment_results_pretrained_effnet = {}\n",
        "\n",
        "# early_stopping_patience_configs = {\n",
        "#     \"es5\": EARLY_STOPPING_PATIENCE_5,\n",
        "#     \"es10\": EARLY_STOPPING_PATIENCE_10,\n",
        "#     \"es15\": EARLY_STOPPING_PATIENCE_15,\n",
        "#     # \"es20\": EARLY_STOPPING_PATIENCE_20,\n",
        "#     \"esNone\": EARLY_STOPPING_PATIENCE_NONE\n",
        "# }\n",
        "\n",
        "# for suffix, patience_val in early_stopping_patience_configs.items():\n",
        "#     all_experiment_results_pretrained_effnet[suffix] = run_pretrained_effnet_experiment_automated_naming(patience_val, suffix,\n",
        "#                                                                                  train_loader_asli_pretrained_efficientnet, val_loader_asli_pretrained_efficientnet, test_loader_asli_pretrained_efficientnet,\n",
        "#                                                                                  train_loader_asli_gan_pretrained_efficientnet, val_loader_asli_gan_pretrained_efficientnet, test_loader_asli_gan_pretrained_efficientnet,\n",
        "#                                                                                  train_loader_asli_gan_ratio_pretrained_efficientnet, val_loader_asli_gan_ratio_pretrained_efficientnet, test_loader_asli_gan_ratio_pretrained_efficientnet,\n",
        "#                                                                                  class_names, NUM_CLASSES, DEVICE, WEIGHT_DECAY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ERYfteFHGrd",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "efficientnetv2b0-lla-asli",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
